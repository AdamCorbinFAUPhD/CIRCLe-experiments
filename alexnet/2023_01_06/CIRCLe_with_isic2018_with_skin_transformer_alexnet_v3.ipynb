{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1_HSnGPDTmtQqL19RJaSZ9opYOjST_pFh",
      "authorship_tag": "ABX9TyPHNfnRHFH2JlSFp9b8kGjf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamCorbinFAUPhD/CIRCLe-experiments/blob/main/alexnet/2023_01_06/CIRCLe_with_isic2018_with_skin_transformer_alexnet_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment notes\n",
        "- date: 2023/01/06 4:42am\n",
        "- base model: alexnet\n",
        "- Adding the de-normlization for input image before transforming the image. then normalizing the transformed image before running it throug the base model\n",
        "\n",
        "\n",
        "Results: \n",
        "- overfitting and validation not good\n",
        "\n"
      ],
      "metadata": {
        "id": "7n3C7hssBDI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro\n",
        "\n",
        "This notebook is used to modify the implementation of CIRCLe from this paper : [CIRCLe: Color Invariant Representation\n",
        "Learning for Unbiased Classification of Skin\n",
        "Lesions](https://arxiv.org/pdf/2208.13528.pdf)\n",
        "\n",
        "Their github repo is : https://github.com/arezou-pakzad/CIRCLe\n",
        "\n",
        "This paper uses the Fitzpatrick17k dataset which can be obtained here: https://github.com/mattgroh/fitzpatrick17k\n",
        "\n",
        "For these set of experiments we will use the ISIC 2017 dataset from: https://github.com/manideep2510/melanoma_segmentation.git "
      ],
      "metadata": {
        "id": "jCpy2CqVJ-VI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TODO list\n",
        "\n",
        "1. [X] Download 2018 dataset\n",
        "1. [X] Analize dataset to get Fitzpatrick info. \n",
        "1. [X] Save off Fitzpatrick info data so we dont have to do it every time\n",
        "1. [X] load cached fitzpatrick data\n",
        "1. [X] Create masks uing https://github.com/DebeshJha/2020-CBMS-DoubleU-Net Because Task 3 for 2018 doesnt havent masks. Trick was to get the higher end GPU and ram (12/29/2022)\n",
        "1. [X] Create pytorch dataloader for ISIC 2018 dataset including loading masks, images, diagnossis, fitzpatrick type for training (12/30/2022) needed to create custom split function\n",
        "1. [X] Create dataloaders for test and validation  (12/30/2022)\n",
        "1. [X] Added jupiter notebook download code into the github repo (1/1/2023)\n",
        "1. [X] plug in dataloader into CIRCLe main file (1/1/2023)\n",
        "1. [X] Figure out how to transform image and mask the same from the dataloader (1/2/2023)\n",
        "1. [X] Use the new dataloader to train the model (1/2/2023)\n",
        "1. [X] Use new transformer for CIRCLe model (1/3/2023)\n",
        "1. [ ] test using different base models\n",
        "1. [ ] test that adding dropout might help with overfitting\n",
        "1. [ ] Add more metrics such as precision and recall\n",
        "1. [ ] add fairness metrics\n",
        "1. [ ] add confusion matrics\n",
        "1. [ ] add sensitivity and specificity\n",
        "1. [ ] add metrics for each class\n",
        "1. [ ] Turn back on the random rotation and flipping in the dataloader\n",
        "1. [ ] (optional) Go back and download and use larger datasets\n",
        "1. [ ] (optional) Run Fitzpatrick on larger datasets(currently using the test set from isic 2018 task 3)\n",
        "1. [ ] The dataloaders need to be split stratified different than the current \"training, validation, and test\" as given from https://challenge.isic-archive.com/data/#2018 based on skin types. 12/30/2022 - I think this is done BUT we might consider doing k-fold approach which adds another layer of complexity to the dataloaders"
      ],
      "metadata": {
        "id": "ajchIOEXMH4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the environment"
      ],
      "metadata": {
        "id": "1IqTnocWPMkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX3pAmwlWnf5",
        "outputId": "41909126-1373-4901-d2ec-4126c405df48"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs & imports"
      ],
      "metadata": {
        "id": "yzsWv7g9MB87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download latest code"
      ],
      "metadata": {
        "id": "cLWa0BAdPQBI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kQ5LposJzeZ",
        "outputId": "29e14e4f-17cb-4c69-84a5-0fde32a7f32e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CIRCLe'...\n",
            "remote: Enumerating objects: 310, done.\u001b[K\n",
            "remote: Counting objects: 100% (310/310), done.\u001b[K\n",
            "remote: Compressing objects: 100% (208/208), done.\u001b[K\n",
            "remote: Total 310 (delta 150), reused 251 (delta 98), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (310/310), 1.78 MiB | 10.67 MiB/s, done.\n",
            "Resolving deltas: 100% (150/150), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/acorbin3/CIRCLe.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ./CIRCLe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9eGp7LOm90J",
        "outputId": "2405ab6d-55db-481f-fee7-5ea295ee8b84"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CIRCLe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout -- models/circle.py"
      ],
      "metadata": {
        "id": "OgCG317Q6qru"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDNNuLRyKQ7-",
        "outputId": "81c8dadc-7e71-4677-e783-f5b3ff46efc6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -r ./requirements.txt"
      ],
      "metadata": {
        "id": "DWzYtBAIKvuD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dbaaf73-ee48-4503-e540-66538f6dc3c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.23.2\n",
            "  Downloading numpy-1.23.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.4.4\n",
            "  Downloading pandas-1.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow==9.2.0\n",
            "  Downloading Pillow-9.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit_learn==1.1.2\n",
            "  Downloading scikit_learn-1.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow==2.9.2 in /usr/local/lib/python3.8/dist-packages (from -r ./requirements.txt (line 5)) (2.9.2)\n",
            "Collecting torch==1.12.1\n",
            "  Downloading torch-1.12.1-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.13.1\n",
            "  Downloading torchvision-0.13.1-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm==4.64.1 in /usr/local/lib/python3.8/dist-packages (from -r ./requirements.txt (line 8)) (4.64.1)\n",
            "Collecting derm-ita>=0.0.8\n",
            "  Downloading derm_ita-0.0.8-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.4.4->-r ./requirements.txt (line 2)) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.4.4->-r ./requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==1.1.2->-r ./requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==1.1.2->-r ./requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==1.1.2->-r ./requirements.txt (line 4)) (1.7.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.51.1)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (2.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (2.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.12)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (3.19.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (21.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (0.29.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.14.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.6.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (14.0.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (2.9.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (2.9.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision==0.13.1->-r ./requirements.txt (line 7)) (2.25.1)\n",
            "Collecting patchify>=0.2.3\n",
            "  Downloading patchify-0.2.3-py3-none-any.whl (6.6 kB)\n",
            "Collecting scikit-image>=0.19\n",
            "  Downloading scikit_image-0.19.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (0.38.4)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.19->derm-ita>=0.0.8->-r ./requirements.txt (line 9)) (2.8.8)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.19->derm-ita>=0.0.8->-r ./requirements.txt (line 9)) (1.4.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.19->derm-ita>=0.0.8->-r ./requirements.txt (line 9)) (2.9.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.19->derm-ita>=0.0.8->-r ./requirements.txt (line 9)) (2022.10.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (3.0.9)\n",
            "Collecting scipy>=1.3.2\n",
            "  Downloading scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (2.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.13.1->-r ./requirements.txt (line 7)) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.13.1->-r ./requirements.txt (line 7)) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.13.1->-r ./requirements.txt (line 7)) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.13.1->-r ./requirements.txt (line 7)) (2022.12.7)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (5.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (3.2.2)\n",
            "Installing collected packages: torch, Pillow, numpy, torchvision, scipy, patchify, pandas, scikit_learn, scikit-image, derm-ita\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0+cu116\n",
            "    Uninstalling torch-1.13.0+cu116:\n",
            "      Successfully uninstalled torch-1.13.0+cu116\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.0+cu116\n",
            "    Uninstalling torchvision-0.14.0+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.0+cu116\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: scikit_learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.18.3\n",
            "    Uninstalling scikit-image-0.18.3:\n",
            "      Successfully uninstalled scikit-image-0.18.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.12.1 which is incompatible.\n",
            "torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-9.2.0 derm-ita-0.0.8 numpy-1.23.2 pandas-1.4.4 patchify-0.2.3 scikit-image-0.19.3 scikit_learn-1.1.2 scipy-1.10.0 torch-1.12.1 torchvision-0.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IF ERROR, RESTART RUNTIME due to derm-ita lib\n",
        "This is due to derm-ita using newer libaries than the Google Colab default(during this time of 12/24/2022)"
      ],
      "metadata": {
        "id": "pB8smeonMesX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train CIRCLe model "
      ],
      "metadata": {
        "id": "fKUV22LhVzBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir ./saved\n",
        "%mkdir ./saved/model"
      ],
      "metadata": {
        "id": "08ngcZp9m1-S"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout -- ./models/circle.py"
      ],
      "metadata": {
        "id": "IvF9sw289-yh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout -- main.py"
      ],
      "metadata": {
        "id": "7sd5djCe4Atk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akY1e2Sh-FdU",
        "outputId": "4a4cd9c6-b1e9-493e-e837-c2be36c8be3b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --use_reg_loss True --base alexnet --dataset isic2018"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH5FKsbclkje",
        "outputId": "b10032e9-ed50-431b-f06d-2103faaf555a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flags:\n",
            "\talpha: 0.1\n",
            "\tbase: alexnet\n",
            "\tbatch_size: 32\n",
            "\tdata_dir: ../data/fitz17k/images/all/\n",
            "\tdataset: isic2018\n",
            "\tepochs: 100\n",
            "\tgan_path: saved/stargan/\n",
            "\thidden_dim: 256\n",
            "\tlr: 0.001\n",
            "\tmodel: circle\n",
            "\tmodel_save_dir: saved/model/\n",
            "\tnum_classes: 7\n",
            "\tseed: 1\n",
            "\tuse_reg_loss: True\n",
            "\tweight_decay: 0.001\n",
            "isic2018 images already downloaded\n",
            "isic 2018 masks already downladed\n",
            "Donloading isic 2018 ground truth classification data\n",
            "Creating dataframe\n",
            "\t Looking for cached dataframe\n",
            "\t\t organize_data/isic_2018/saved_data_2022_12_27_isic_2018.csv\n",
            "Creating dataframe. Complete!\n",
            "Splitting up the dataset into train,test, validation datasets\n",
            "fizpatrick_skin_type: 1 8001\n",
            "\t train 6400\n",
            "\t test 800\n",
            "\t val 801\n",
            "fizpatrick_skin_type: 2 1049\n",
            "\t train 839\n",
            "\t test 105\n",
            "\t val 105\n",
            "fizpatrick_skin_type: 3 513\n",
            "\t train 410\n",
            "\t test 51\n",
            "\t val 52\n",
            "fizpatrick_skin_type: 4 182\n",
            "\t train 145\n",
            "\t test 18\n",
            "\t val 19\n",
            "fizpatrick_skin_type: 5 107\n",
            "\t train 85\n",
            "\t test 11\n",
            "\t val 11\n",
            "fizpatrick_skin_type: 6 163\n",
            "\t train 130\n",
            "\t test 16\n",
            "\t val 17\n",
            "total_train: 8009 79.9700449326011\n",
            "total_test: 1001 9.995007488766849\n",
            "total_val: 1005 10.034947578632051\n",
            "train size: 8009\n",
            "test size: 1001\n",
            "val size: 1005\n",
            "\ttrain: skin type 1 : 6400\n",
            "\ttrain: skin type 2 : 839\n",
            "\ttrain: skin type 3 : 410\n",
            "\ttrain: skin type 4 : 145\n",
            "\ttrain: skin type 5 : 85\n",
            "\ttrain: skin type 6 : 130\n",
            "----\n",
            "\ttest: skin type 1 : 800\n",
            "\ttest: skin type 2 : 105\n",
            "\ttest: skin type 3 : 51\n",
            "\ttest: skin type 4 : 18\n",
            "\ttest: skin type 5 : 11\n",
            "\ttest: skin type 6 : 16\n",
            "----\n",
            "\tval: skin type 1 : 801\n",
            "\tval: skin type 2 : 105\n",
            "\tval: skin type 3 : 52\n",
            "\tval: skin type 4 : 19\n",
            "\tval: skin type 5 : 11\n",
            "\tval: skin type 6 : 17\n",
            "train size: 8009\n",
            "val size: 1005\n",
            "train skin types: [1 2 3 4 5 6]\n",
            "val skin types: [1 2 3 4 5 6]\n",
            "train skin conditions: 7\n",
            "val skin conditions: 7\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Epoch 0: Best val loss inf, Best val acc 0.0, best val recall 0, best val precision 0\n",
            ">>> Training: Loss 1.166, Reg 0.009, Acc 0.661, precision: 0.661, recall0.661\n",
            ">>> Val: Loss 1.167, Reg 0.000, Acc 0.662, precision: 0.662, recall0.662\n",
            "Saved model with highest acc ...\n",
            "Epoch 1: Best val loss 1.1665170884901477, Best val acc 0.6622983870967742, best val recall 0.6622983870967742, best val precision 0.6622983870967742\n",
            ">>> Training: Loss 1.132, Reg 0.003, Acc 0.667, precision: 0.667, recall0.667\n",
            ">>> Val: Loss 1.174, Reg 0.000, Acc 0.661, precision: 0.661, recall0.661\n",
            "Epoch 2: Best val loss 1.1665170884901477, Best val acc 0.6622983870967742, best val recall 0.6622983870967742, best val precision 0.6622983870967742\n",
            ">>> Training: Loss 1.129, Reg 0.003, Acc 0.667, precision: 0.667, recall0.667\n",
            ">>> Val: Loss 1.195, Reg 0.000, Acc 0.660, precision: 0.660, recall0.660\n",
            "Epoch 3: Best val loss 1.1665170884901477, Best val acc 0.6622983870967742, best val recall 0.6622983870967742, best val precision 0.6622983870967742\n",
            ">>> Training: Loss 1.122, Reg 0.002, Acc 0.667, precision: 0.667, recall0.667\n",
            ">>> Val: Loss 1.203, Reg 0.000, Acc 0.663, precision: 0.663, recall0.663\n",
            "Saved model with highest acc ...\n",
            "Epoch 4: Best val loss 1.1665170884901477, Best val acc 0.6633064516129032, best val recall 0.6633064516129032, best val precision 0.6633064516129032\n",
            ">>> Training: Loss 1.114, Reg 0.002, Acc 0.667, precision: 0.667, recall0.667\n",
            ">>> Val: Loss 1.219, Reg 0.000, Acc 0.662, precision: 0.662, recall0.662\n",
            "Epoch 5: Best val loss 1.1665170884901477, Best val acc 0.6633064516129032, best val recall 0.6633064516129032, best val precision 0.6633064516129032\n",
            ">>> Training: Loss 1.110, Reg 0.002, Acc 0.667, precision: 0.667, recall0.667\n",
            ">>> Val: Loss 1.231, Reg 0.000, Acc 0.664, precision: 0.664, recall0.664\n",
            "Saved model with highest acc ...\n",
            "Epoch 6: Best val loss 1.1665170884901477, Best val acc 0.6643145161290323, best val recall 0.6643145161290323, best val precision 0.6643145161290323\n",
            ">>> Training: Loss 1.102, Reg 0.002, Acc 0.667, precision: 0.667, recall0.667\n",
            ">>> Val: Loss 1.228, Reg 0.000, Acc 0.665, precision: 0.665, recall0.665\n",
            "Saved model with highest acc ...\n",
            "Epoch 7: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 1.094, Reg 0.002, Acc 0.667, precision: 0.667, recall0.667\n",
            ">>> Val: Loss 1.236, Reg 0.000, Acc 0.664, precision: 0.664, recall0.664\n",
            "Epoch 8: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 1.085, Reg 0.003, Acc 0.667, precision: 0.667, recall0.667\n",
            ">>> Val: Loss 1.248, Reg 0.000, Acc 0.663, precision: 0.663, recall0.663\n",
            "Epoch 9: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 1.072, Reg 0.003, Acc 0.667, precision: 0.667, recall0.667\n",
            ">>> Val: Loss 1.264, Reg 0.000, Acc 0.661, precision: 0.661, recall0.661\n",
            "Epoch 10: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 1.059, Reg 0.003, Acc 0.667, precision: 0.667, recall0.667\n",
            ">>> Val: Loss 1.269, Reg 0.000, Acc 0.664, precision: 0.664, recall0.664\n",
            "Epoch 11: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 1.042, Reg 0.003, Acc 0.667, precision: 0.667, recall0.667\n",
            ">>> Val: Loss 1.251, Reg 0.000, Acc 0.665, precision: 0.665, recall0.665\n",
            "Epoch 12: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 1.023, Reg 0.003, Acc 0.668, precision: 0.668, recall0.668\n",
            ">>> Val: Loss 1.279, Reg 0.000, Acc 0.662, precision: 0.662, recall0.662\n",
            "Epoch 13: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 1.000, Reg 0.004, Acc 0.670, precision: 0.670, recall0.670\n",
            ">>> Val: Loss 1.281, Reg 0.000, Acc 0.665, precision: 0.665, recall0.665\n",
            "Epoch 14: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.984, Reg 0.004, Acc 0.673, precision: 0.673, recall0.673\n",
            ">>> Val: Loss 1.310, Reg 0.000, Acc 0.661, precision: 0.661, recall0.661\n",
            "Epoch 15: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.961, Reg 0.005, Acc 0.677, precision: 0.677, recall0.677\n",
            ">>> Val: Loss 1.344, Reg 0.000, Acc 0.654, precision: 0.654, recall0.654\n",
            "Epoch 16: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.931, Reg 0.005, Acc 0.685, precision: 0.685, recall0.685\n",
            ">>> Val: Loss 1.383, Reg 0.000, Acc 0.651, precision: 0.651, recall0.651\n",
            "Epoch 17: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.906, Reg 0.006, Acc 0.694, precision: 0.694, recall0.694\n",
            ">>> Val: Loss 1.446, Reg 0.000, Acc 0.645, precision: 0.645, recall0.645\n",
            "Epoch 18: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.891, Reg 0.006, Acc 0.696, precision: 0.696, recall0.696\n",
            ">>> Val: Loss 1.489, Reg 0.000, Acc 0.642, precision: 0.642, recall0.642\n",
            "Epoch 19: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.866, Reg 0.007, Acc 0.706, precision: 0.706, recall0.706\n",
            ">>> Val: Loss 1.556, Reg 0.000, Acc 0.635, precision: 0.635, recall0.635\n",
            "Epoch 20: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.841, Reg 0.007, Acc 0.715, precision: 0.715, recall0.715\n",
            ">>> Val: Loss 1.661, Reg 0.000, Acc 0.643, precision: 0.643, recall0.643\n",
            "Epoch 21: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.809, Reg 0.008, Acc 0.722, precision: 0.722, recall0.722\n",
            ">>> Val: Loss 1.732, Reg 0.000, Acc 0.638, precision: 0.638, recall0.638\n",
            "Epoch 22: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.792, Reg 0.009, Acc 0.727, precision: 0.727, recall0.727\n",
            ">>> Val: Loss 1.789, Reg 0.000, Acc 0.626, precision: 0.626, recall0.626\n",
            "Epoch 23: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.781, Reg 0.008, Acc 0.731, precision: 0.731, recall0.731\n",
            ">>> Val: Loss 1.684, Reg 0.000, Acc 0.610, precision: 0.610, recall0.610\n",
            "Epoch 24: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.789, Reg 0.008, Acc 0.731, precision: 0.731, recall0.731\n",
            ">>> Val: Loss 1.592, Reg 0.000, Acc 0.616, precision: 0.616, recall0.616\n",
            "Epoch 25: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.780, Reg 0.008, Acc 0.735, precision: 0.735, recall0.735\n",
            ">>> Val: Loss 1.765, Reg 0.000, Acc 0.642, precision: 0.642, recall0.642\n",
            "Epoch 26: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.764, Reg 0.008, Acc 0.743, precision: 0.743, recall0.743\n",
            ">>> Val: Loss 1.650, Reg 0.000, Acc 0.643, precision: 0.643, recall0.643\n",
            "Epoch 27: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.753, Reg 0.008, Acc 0.743, precision: 0.743, recall0.743\n",
            ">>> Val: Loss 1.643, Reg 0.000, Acc 0.633, precision: 0.633, recall0.633\n",
            "Epoch 28: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.704, Reg 0.009, Acc 0.759, precision: 0.759, recall0.759\n",
            ">>> Val: Loss 1.802, Reg 0.000, Acc 0.651, precision: 0.651, recall0.651\n",
            "Epoch 29: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.662, Reg 0.010, Acc 0.774, precision: 0.774, recall0.774\n",
            ">>> Val: Loss 1.773, Reg 0.000, Acc 0.655, precision: 0.655, recall0.655\n",
            "Epoch 30: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.632, Reg 0.010, Acc 0.775, precision: 0.775, recall0.775\n",
            ">>> Val: Loss 2.142, Reg 0.000, Acc 0.657, precision: 0.657, recall0.657\n",
            "Epoch 31: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.603, Reg 0.011, Acc 0.790, precision: 0.790, recall0.790\n",
            ">>> Val: Loss 1.973, Reg 0.000, Acc 0.639, precision: 0.639, recall0.639\n",
            "Epoch 32: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.577, Reg 0.011, Acc 0.797, precision: 0.797, recall0.797\n",
            ">>> Val: Loss 2.149, Reg 0.000, Acc 0.645, precision: 0.645, recall0.645\n",
            "Epoch 33: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.565, Reg 0.011, Acc 0.805, precision: 0.805, recall0.805\n",
            ">>> Val: Loss 2.373, Reg 0.000, Acc 0.659, precision: 0.659, recall0.659\n",
            "Epoch 34: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.555, Reg 0.012, Acc 0.806, precision: 0.806, recall0.806\n",
            ">>> Val: Loss 2.430, Reg 0.000, Acc 0.654, precision: 0.654, recall0.654\n",
            "Epoch 35: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.537, Reg 0.012, Acc 0.813, precision: 0.813, recall0.813\n",
            ">>> Val: Loss 2.149, Reg 0.000, Acc 0.645, precision: 0.645, recall0.645\n",
            "Epoch 36: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.512, Reg 0.012, Acc 0.824, precision: 0.824, recall0.824\n",
            ">>> Val: Loss 2.174, Reg 0.000, Acc 0.635, precision: 0.635, recall0.635\n",
            "Epoch 37: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.506, Reg 0.012, Acc 0.823, precision: 0.823, recall0.823\n",
            ">>> Val: Loss 2.235, Reg 0.000, Acc 0.638, precision: 0.638, recall0.638\n",
            "Epoch 38: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.474, Reg 0.012, Acc 0.836, precision: 0.836, recall0.836\n",
            ">>> Val: Loss 2.520, Reg 0.000, Acc 0.646, precision: 0.646, recall0.646\n",
            "Epoch 39: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.446, Reg 0.012, Acc 0.849, precision: 0.849, recall0.849\n",
            ">>> Val: Loss 2.519, Reg 0.000, Acc 0.641, precision: 0.641, recall0.641\n",
            "Epoch 40: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.428, Reg 0.013, Acc 0.853, precision: 0.853, recall0.853\n",
            ">>> Val: Loss 2.690, Reg 0.000, Acc 0.653, precision: 0.653, recall0.653\n",
            "Epoch 41: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.401, Reg 0.013, Acc 0.864, precision: 0.864, recall0.864\n",
            ">>> Val: Loss 2.624, Reg 0.000, Acc 0.629, precision: 0.629, recall0.629\n",
            "Epoch 42: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.383, Reg 0.013, Acc 0.868, precision: 0.868, recall0.868\n",
            ">>> Val: Loss 2.639, Reg 0.000, Acc 0.627, precision: 0.627, recall0.627\n",
            "Epoch 43: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.379, Reg 0.013, Acc 0.876, precision: 0.876, recall0.876\n",
            ">>> Val: Loss 2.307, Reg 0.000, Acc 0.621, precision: 0.621, recall0.621\n",
            "Epoch 44: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.383, Reg 0.012, Acc 0.873, precision: 0.873, recall0.873\n",
            ">>> Val: Loss 2.635, Reg 0.000, Acc 0.643, precision: 0.643, recall0.643\n",
            "Epoch 45: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.359, Reg 0.012, Acc 0.878, precision: 0.878, recall0.878\n",
            ">>> Val: Loss 3.031, Reg 0.000, Acc 0.645, precision: 0.645, recall0.645\n",
            "Epoch 46: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.343, Reg 0.013, Acc 0.884, precision: 0.884, recall0.884\n",
            ">>> Val: Loss 2.710, Reg 0.000, Acc 0.607, precision: 0.607, recall0.607\n",
            "Epoch 47: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.335, Reg 0.012, Acc 0.888, precision: 0.888, recall0.888\n",
            ">>> Val: Loss 2.979, Reg 0.000, Acc 0.653, precision: 0.653, recall0.653\n",
            "Epoch 48: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.318, Reg 0.012, Acc 0.893, precision: 0.893, recall0.893\n",
            ">>> Val: Loss 2.862, Reg 0.000, Acc 0.623, precision: 0.623, recall0.623\n",
            "Epoch 49: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.284, Reg 0.013, Acc 0.905, precision: 0.905, recall0.905\n",
            ">>> Val: Loss 3.096, Reg 0.000, Acc 0.643, precision: 0.643, recall0.643\n",
            "Epoch 50: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.276, Reg 0.013, Acc 0.907, precision: 0.907, recall0.907\n",
            ">>> Val: Loss 3.228, Reg 0.000, Acc 0.649, precision: 0.649, recall0.649\n",
            "Epoch 51: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.248, Reg 0.014, Acc 0.916, precision: 0.916, recall0.916\n",
            ">>> Val: Loss 3.135, Reg 0.000, Acc 0.640, precision: 0.640, recall0.640\n",
            "Epoch 52: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.238, Reg 0.013, Acc 0.921, precision: 0.921, recall0.921\n",
            ">>> Val: Loss 3.515, Reg 0.000, Acc 0.652, precision: 0.652, recall0.652\n",
            "Epoch 53: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.222, Reg 0.013, Acc 0.928, precision: 0.928, recall0.928\n",
            ">>> Val: Loss 3.323, Reg 0.000, Acc 0.648, precision: 0.648, recall0.648\n",
            "Epoch 54: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.227, Reg 0.013, Acc 0.924, precision: 0.924, recall0.924\n",
            ">>> Val: Loss 3.360, Reg 0.000, Acc 0.643, precision: 0.643, recall0.643\n",
            "Epoch 55: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.200, Reg 0.013, Acc 0.934, precision: 0.934, recall0.934\n",
            ">>> Val: Loss 3.741, Reg 0.000, Acc 0.645, precision: 0.645, recall0.645\n",
            "Epoch 56: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.178, Reg 0.014, Acc 0.944, precision: 0.944, recall0.944\n",
            ">>> Val: Loss 3.782, Reg 0.000, Acc 0.651, precision: 0.651, recall0.651\n",
            "Epoch 57: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.162, Reg 0.013, Acc 0.943, precision: 0.943, recall0.943\n",
            ">>> Val: Loss 4.036, Reg 0.000, Acc 0.645, precision: 0.645, recall0.645\n",
            "Epoch 58: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.157, Reg 0.013, Acc 0.950, precision: 0.950, recall0.950\n",
            ">>> Val: Loss 4.321, Reg 0.000, Acc 0.650, precision: 0.650, recall0.650\n",
            "Epoch 59: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.158, Reg 0.013, Acc 0.947, precision: 0.947, recall0.947\n",
            ">>> Val: Loss 3.803, Reg 0.000, Acc 0.655, precision: 0.655, recall0.655\n",
            "Epoch 60: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.149, Reg 0.013, Acc 0.952, precision: 0.952, recall0.952\n",
            ">>> Val: Loss 3.733, Reg 0.000, Acc 0.635, precision: 0.635, recall0.635\n",
            "Epoch 61: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.147, Reg 0.013, Acc 0.953, precision: 0.953, recall0.953\n",
            ">>> Val: Loss 4.022, Reg 0.000, Acc 0.644, precision: 0.644, recall0.644\n",
            "Epoch 62: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.130, Reg 0.013, Acc 0.957, precision: 0.957, recall0.957\n",
            ">>> Val: Loss 3.688, Reg 0.000, Acc 0.636, precision: 0.636, recall0.636\n",
            "Epoch 63: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.121, Reg 0.013, Acc 0.961, precision: 0.961, recall0.961\n",
            ">>> Val: Loss 3.686, Reg 0.000, Acc 0.636, precision: 0.636, recall0.636\n",
            "Epoch 64: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.104, Reg 0.014, Acc 0.966, precision: 0.966, recall0.966\n",
            ">>> Val: Loss 3.771, Reg 0.000, Acc 0.630, precision: 0.630, recall0.630\n",
            "Epoch 65: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.104, Reg 0.014, Acc 0.968, precision: 0.968, recall0.968\n",
            ">>> Val: Loss 3.738, Reg 0.000, Acc 0.630, precision: 0.630, recall0.630\n",
            "Epoch 66: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.091, Reg 0.013, Acc 0.974, precision: 0.974, recall0.974\n",
            ">>> Val: Loss 3.863, Reg 0.000, Acc 0.633, precision: 0.633, recall0.633\n",
            "Epoch 67: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.084, Reg 0.013, Acc 0.974, precision: 0.974, recall0.974\n",
            ">>> Val: Loss 3.972, Reg 0.000, Acc 0.648, precision: 0.648, recall0.648\n",
            "Epoch 68: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.074, Reg 0.013, Acc 0.978, precision: 0.978, recall0.978\n",
            ">>> Val: Loss 3.982, Reg 0.000, Acc 0.643, precision: 0.643, recall0.643\n",
            "Epoch 69: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.071, Reg 0.013, Acc 0.979, precision: 0.979, recall0.979\n",
            ">>> Val: Loss 3.710, Reg 0.000, Acc 0.633, precision: 0.633, recall0.633\n",
            "Epoch 70: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.068, Reg 0.013, Acc 0.979, precision: 0.979, recall0.979\n",
            ">>> Val: Loss 3.848, Reg 0.000, Acc 0.637, precision: 0.637, recall0.637\n",
            "Epoch 71: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.066, Reg 0.013, Acc 0.982, precision: 0.982, recall0.982\n",
            ">>> Val: Loss 3.634, Reg 0.000, Acc 0.631, precision: 0.631, recall0.631\n",
            "Epoch 72: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.064, Reg 0.013, Acc 0.983, precision: 0.983, recall0.983\n",
            ">>> Val: Loss 3.739, Reg 0.000, Acc 0.631, precision: 0.631, recall0.631\n",
            "Epoch 73: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.053, Reg 0.013, Acc 0.985, precision: 0.985, recall0.985\n",
            ">>> Val: Loss 3.668, Reg 0.000, Acc 0.644, precision: 0.644, recall0.644\n",
            "Epoch 74: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.042, Reg 0.013, Acc 0.991, precision: 0.991, recall0.991\n",
            ">>> Val: Loss 3.602, Reg 0.000, Acc 0.628, precision: 0.628, recall0.628\n",
            "Epoch 75: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.043, Reg 0.013, Acc 0.988, precision: 0.988, recall0.988\n",
            ">>> Val: Loss 3.509, Reg 0.000, Acc 0.636, precision: 0.636, recall0.636\n",
            "Epoch 76: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.038, Reg 0.012, Acc 0.990, precision: 0.990, recall0.990\n",
            ">>> Val: Loss 3.682, Reg 0.000, Acc 0.636, precision: 0.636, recall0.636\n",
            "Epoch 77: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.033, Reg 0.013, Acc 0.991, precision: 0.991, recall0.991\n",
            ">>> Val: Loss 3.442, Reg 0.000, Acc 0.627, precision: 0.627, recall0.627\n",
            "Epoch 78: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.035, Reg 0.012, Acc 0.991, precision: 0.991, recall0.991\n",
            ">>> Val: Loss 3.974, Reg 0.000, Acc 0.640, precision: 0.640, recall0.640\n",
            "Epoch 79: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.035, Reg 0.012, Acc 0.991, precision: 0.991, recall0.991\n",
            ">>> Val: Loss 3.764, Reg 0.000, Acc 0.641, precision: 0.641, recall0.641\n",
            "Epoch 80: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.026, Reg 0.012, Acc 0.995, precision: 0.995, recall0.995\n",
            ">>> Val: Loss 3.858, Reg 0.000, Acc 0.642, precision: 0.642, recall0.642\n",
            "Epoch 81: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.026, Reg 0.012, Acc 0.995, precision: 0.995, recall0.995\n",
            ">>> Val: Loss 3.661, Reg 0.000, Acc 0.636, precision: 0.636, recall0.636\n",
            "Epoch 82: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.026, Reg 0.012, Acc 0.995, precision: 0.995, recall0.995\n",
            ">>> Val: Loss 3.878, Reg 0.000, Acc 0.636, precision: 0.636, recall0.636\n",
            "Epoch 83: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.023, Reg 0.012, Acc 0.995, precision: 0.995, recall0.995\n",
            ">>> Val: Loss 3.755, Reg 0.000, Acc 0.628, precision: 0.628, recall0.628\n",
            "Epoch 84: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.024, Reg 0.012, Acc 0.994, precision: 0.994, recall0.994\n",
            ">>> Val: Loss 3.866, Reg 0.000, Acc 0.639, precision: 0.639, recall0.639\n",
            "Epoch 85: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.025, Reg 0.011, Acc 0.994, precision: 0.994, recall0.994\n",
            ">>> Val: Loss 3.799, Reg 0.000, Acc 0.643, precision: 0.643, recall0.643\n",
            "Epoch 86: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.024, Reg 0.011, Acc 0.994, precision: 0.994, recall0.994\n",
            ">>> Val: Loss 3.466, Reg 0.000, Acc 0.638, precision: 0.638, recall0.638\n",
            "Epoch 87: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.024, Reg 0.010, Acc 0.995, precision: 0.995, recall0.995\n",
            ">>> Val: Loss 3.622, Reg 0.000, Acc 0.631, precision: 0.631, recall0.631\n",
            "Epoch 88: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.021, Reg 0.010, Acc 0.996, precision: 0.996, recall0.996\n",
            ">>> Val: Loss 3.722, Reg 0.000, Acc 0.639, precision: 0.639, recall0.639\n",
            "Epoch 89: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.015, Reg 0.011, Acc 0.998, precision: 0.998, recall0.998\n",
            ">>> Val: Loss 3.712, Reg 0.000, Acc 0.639, precision: 0.639, recall0.639\n",
            "Epoch 90: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.020, Reg 0.010, Acc 0.996, precision: 0.996, recall0.996\n",
            ">>> Val: Loss 3.661, Reg 0.000, Acc 0.637, precision: 0.637, recall0.637\n",
            "Epoch 91: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.018, Reg 0.010, Acc 0.997, precision: 0.997, recall0.997\n",
            ">>> Val: Loss 4.052, Reg 0.000, Acc 0.645, precision: 0.645, recall0.645\n",
            "Epoch 92: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.019, Reg 0.010, Acc 0.997, precision: 0.997, recall0.997\n",
            ">>> Val: Loss 3.796, Reg 0.000, Acc 0.644, precision: 0.644, recall0.644\n",
            "Epoch 93: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.017, Reg 0.010, Acc 0.996, precision: 0.996, recall0.996\n",
            ">>> Val: Loss 3.786, Reg 0.000, Acc 0.639, precision: 0.639, recall0.639\n",
            "Epoch 94: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.015, Reg 0.010, Acc 0.997, precision: 0.997, recall0.997\n",
            ">>> Val: Loss 3.548, Reg 0.000, Acc 0.631, precision: 0.631, recall0.631\n",
            "Epoch 95: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.018, Reg 0.009, Acc 0.997, precision: 0.997, recall0.997\n",
            ">>> Val: Loss 3.735, Reg 0.000, Acc 0.642, precision: 0.642, recall0.642\n",
            "Epoch 96: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.014, Reg 0.009, Acc 0.998, precision: 0.998, recall0.998\n",
            ">>> Val: Loss 3.828, Reg 0.000, Acc 0.644, precision: 0.644, recall0.644\n",
            "Epoch 97: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.015, Reg 0.009, Acc 0.997, precision: 0.997, recall0.997\n",
            ">>> Val: Loss 3.566, Reg 0.000, Acc 0.639, precision: 0.639, recall0.639\n",
            "Epoch 98: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.012, Reg 0.010, Acc 0.998, precision: 0.998, recall0.998\n",
            ">>> Val: Loss 3.847, Reg 0.000, Acc 0.646, precision: 0.646, recall0.646\n",
            "Epoch 99: Best val loss 1.1665170884901477, Best val acc 0.6653225806451613, best val recall 0.6653225806451613, best val precision 0.6653225806451613\n",
            ">>> Training: Loss 0.012, Reg 0.009, Acc 0.998, precision: 0.998, recall0.998\n",
            ">>> Val: Loss 3.786, Reg 0.000, Acc 0.641, precision: 0.641, recall0.641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%mkdir /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/model_checkpoints"
      ],
      "metadata": {
        "id": "O4m8fvzN-oJN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%cp ./saved/model/epoch97_acc_0.762.ckpt /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/model_checkpoints/CIRCLE/mobilenetv3l/"
      ],
      "metadata": {
        "id": "1hFw2rQB-7np"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}