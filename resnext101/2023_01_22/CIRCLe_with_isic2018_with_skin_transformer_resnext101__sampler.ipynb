{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamCorbinFAUPhD/CIRCLe-experiments/blob/main/resnext101/2023_01_22/CIRCLe_with_isic2018_with_skin_transformer_resnext101__sampler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K9KcPXtTmUh"
      },
      "source": [
        "# Experiment notes\n",
        "\n",
        "- date: 2023/01/22 5am\n",
        "- base model: resnext101\n",
        "- Using Random sample to balance datasets\n",
        "\n",
        "\n",
        "Results: \n",
        "-  Doesnt seem to help validation much at all\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCpy2CqVJ-VI"
      },
      "source": [
        "# Intro\n",
        "\n",
        "This notebook is used to modify the implementation of CIRCLe from this paper : [CIRCLe: Color Invariant Representation\n",
        "Learning for Unbiased Classification of Skin\n",
        "Lesions](https://arxiv.org/pdf/2208.13528.pdf)\n",
        "\n",
        "Their github repo is : https://github.com/arezou-pakzad/CIRCLe\n",
        "\n",
        "This paper uses the Fitzpatrick17k dataset which can be obtained here: https://github.com/mattgroh/fitzpatrick17k\n",
        "\n",
        "For these set of experiments we will use the ISIC 2017 dataset from: https://github.com/manideep2510/melanoma_segmentation.git "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajchIOEXMH4u"
      },
      "source": [
        "#TODO list\n",
        "\n",
        "1. [X] Download 2018 dataset\n",
        "1. [X] Analize dataset to get Fitzpatrick info. \n",
        "1. [X] Save off Fitzpatrick info data so we dont have to do it every time\n",
        "1. [X] load cached fitzpatrick data\n",
        "1. [X] Create masks uing https://github.com/DebeshJha/2020-CBMS-DoubleU-Net Because Task 3 for 2018 doesnt havent masks. Trick was to get the higher end GPU and ram (12/29/2022)\n",
        "1. [X] Create pytorch dataloader for ISIC 2018 dataset including loading masks, images, diagnossis, fitzpatrick type for training (12/30/2022) needed to create custom split function\n",
        "1. [X] Create dataloaders for test and validation  (12/30/2022)\n",
        "1. [X] Added jupiter notebook download code into the github repo (1/1/2023)\n",
        "1. [X] plug in dataloader into CIRCLe main file (1/1/2023)\n",
        "1. [X] Figure out how to transform image and mask the same from the dataloader (1/2/2023)\n",
        "1. [X] Use the new dataloader to train the model (1/2/2023)\n",
        "1. [X] Use new transformer for CIRCLe model (1/3/2023)\n",
        "1. [ ] test using different base models\n",
        "1. [ ] test that adding dropout might help with overfitting\n",
        "1. [X] Add more metrics such as precision and recall (1/4/2023)\n",
        "1. [ ] add fairness metrics\n",
        "1. [ ] add confusion matrics\n",
        "1. [ ] add sensitivity and specificity\n",
        "1. [ ] add metrics for each class\n",
        "1. [ ] (optional) Go back and download and use larger datasets\n",
        "1. [ ] (optional) Run Fitzpatrick on larger datasets(currently using the test set from isic 2018 task 3)\n",
        "1. [ ] The dataloaders need to be split stratified different than the current \"training, validation, and test\" as given from https://challenge.isic-archive.com/data/#2018 based on skin types. 12/30/2022 - I think this is done BUT we might consider doing k-fold approach which adds another layer of complexity to the dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IqTnocWPMkG"
      },
      "source": [
        "# Set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX3pAmwlWnf5",
        "outputId": "83df9358-98cf-4633-e563-5cce787a1567"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.8.10\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzsWv7g9MB87"
      },
      "source": [
        "## Installs & imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLWa0BAdPQBI"
      },
      "source": [
        "## Download latest code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kQ5LposJzeZ",
        "outputId": "50d80471-5433-4c10-f6d9-b6706a934460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'CIRCLe'...\n",
            "remote: Enumerating objects: 620, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 620 (delta 8), reused 17 (delta 4), pack-reused 597\u001b[K\n",
            "Receiving objects: 100% (620/620), 1.87 MiB | 11.27 MiB/s, done.\n",
            "Resolving deltas: 100% (316/316), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/acorbin3/CIRCLe.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9eGp7LOm90J",
        "outputId": "f12a3108-7b6a-4cf3-b52d-96e2b7425154"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/CIRCLe\n"
          ]
        }
      ],
      "source": [
        "%cd ./CIRCLe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgCG317Q6qru"
      },
      "outputs": [],
      "source": [
        "!git checkout -- models/circle.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDNNuLRyKQ7-",
        "outputId": "fc0f219c-dff0-45a7-f0be-350141c6ee45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWzYtBAIKvuD",
        "outputId": "532717d1-b2bc-4f7d-95c8-0048bbae6d17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.23.2\n",
            "  Downloading numpy-1.23.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.4.4\n",
            "  Downloading pandas-1.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow==9.2.0\n",
            "  Downloading Pillow-9.2.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit_learn==1.1.2\n",
            "  Downloading scikit_learn-1.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow==2.9.2 in /usr/local/lib/python3.8/dist-packages (from -r ./requirements.txt (line 5)) (2.9.2)\n",
            "Collecting torch==1.12.1\n",
            "  Downloading torch-1.12.1-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.13.1\n",
            "  Downloading torchvision-0.13.1-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm==4.64.1 in /usr/local/lib/python3.8/dist-packages (from -r ./requirements.txt (line 8)) (4.64.1)\n",
            "Collecting derm-ita>=0.0.8\n",
            "  Downloading derm_ita-0.0.8-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.8/dist-packages (from -r ./requirements.txt (line 10)) (0.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.4.4->-r ./requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.4.4->-r ./requirements.txt (line 2)) (2022.7)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==1.1.2->-r ./requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==1.1.2->-r ./requirements.txt (line 4)) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit_learn==1.1.2->-r ./requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.12)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (2.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (21.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (3.19.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (4.4.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (2.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (15.0.6.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.51.1)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (2.9.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (0.29.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.1.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision==0.13.1->-r ./requirements.txt (line 7)) (2.25.1)\n",
            "Collecting scikit-image>=0.19\n",
            "  Downloading scikit_image-0.19.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting patchify>=0.2.3\n",
            "  Downloading patchify-0.2.3-py3-none-any.whl (6.6 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (0.38.4)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.19->derm-ita>=0.0.8->-r ./requirements.txt (line 9)) (2022.10.10)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.19->derm-ita>=0.0.8->-r ./requirements.txt (line 9)) (3.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.19->derm-ita>=0.0.8->-r ./requirements.txt (line 9)) (1.4.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.19->derm-ita>=0.0.8->-r ./requirements.txt (line 9)) (2.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (3.0.9)\n",
            "Collecting scipy>=1.3.2\n",
            "  Downloading scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (2.16.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (0.6.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.13.1->-r ./requirements.txt (line 7)) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.13.1->-r ./requirements.txt (line 7)) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.13.1->-r ./requirements.txt (line 7)) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision==0.13.1->-r ./requirements.txt (line 7)) (1.24.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (5.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (6.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.2->-r ./requirements.txt (line 5)) (3.2.2)\n",
            "Installing collected packages: torch, Pillow, numpy, torchvision, scipy, patchify, pandas, scikit_learn, scikit-image, derm-ita\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.1+cu116\n",
            "    Uninstalling torchvision-0.14.1+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.1+cu116\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: scikit_learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.18.3\n",
            "    Uninstalling scikit-image-0.18.3:\n",
            "      Successfully uninstalled scikit-image-0.18.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.12.1 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-9.2.0 derm-ita-0.0.8 numpy-1.23.2 pandas-1.4.4 patchify-0.2.3 scikit-image-0.19.3 scikit_learn-1.1.2 scipy-1.10.0 torch-1.12.1 torchvision-0.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -r ./requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q3ovxukTHlL"
      },
      "source": [
        "**This next block of code will be needed if you get this error: **\n",
        "\n",
        "A100-SXM4-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation. The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhckJLuHSlkS"
      },
      "outputs": [],
      "source": [
        "#!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB8smeonMesX"
      },
      "source": [
        "# IF ERROR, RESTART RUNTIME due to derm-ita lib\n",
        "This is due to derm-ita using newer libaries than the Google Colab default(during this time of 12/24/2022)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKUV22LhVzBy"
      },
      "source": [
        "# Train CIRCLe model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08ngcZp9m1-S"
      },
      "outputs": [],
      "source": [
        "%mkdir ./saved\n",
        "%mkdir ./saved/model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8uJMyX54Qti"
      },
      "outputs": [],
      "source": [
        "!git checkout -- main.py\n",
        "!git checkout -- datasets/isic_2018/dataset.py\n",
        "!git checkout -- models/base.py\n",
        "!git checkout -- Metrics.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akY1e2Sh-FdU",
        "outputId": "495b8661-370a-44ab-e7a9-419ca2c71b7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH5FKsbclkje",
        "outputId": "f01aaf25-23e7-47c7-d580-6fbaca1a16d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Flags:\n",
            "\talpha: 0.1\n",
            "\tbase: resnext101_32x8d\n",
            "\tbatch_size: 32\n",
            "\tdata_dir: ../data/fitz17k/images/all/\n",
            "\tdataset: isic2018\n",
            "\tepochs: 100\n",
            "\tgan_path: saved/stargan/\n",
            "\thidden_dim: 16\n",
            "\tlr: 0.001\n",
            "\tmodel: circle\n",
            "\tmodel_save_dir: saved/model/\n",
            "\tnum_classes: 7\n",
            "\tseed: 1\n",
            "\tuse_reg_loss: True\n",
            "\tweight_decay: 0.001\n",
            "isic2018 images already downloaded\n",
            "isic 2018 masks already downladed\n",
            "Donloading isic 2018 ground truth classification data\n",
            "Creating dataframe\n",
            "\t Looking for cached dataframe\n",
            "\t\t datasets/isic_2018/saved_data_2022_12_27_isic_2018.csv\n",
            "Creating dataframe. Complete!\n",
            "Splitting up the dataset into train,test, validation datasets based on the skin condition\n",
            "fizpatrick_skin_type: ('AKIEC', 1) 291\n",
            "\t train 232\n",
            "\t test 59\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('AKIEC', 2) 29\n",
            "\t train 23\n",
            "\t test 6\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('AKIEC', 3) 4\n",
            "\t train 3\n",
            "\t test 1\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('AKIEC', 4) 1\n",
            "\t train 3\n",
            "\t test 0\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('AKIEC', 6) 2\n",
            "\t train 1\n",
            "\t test 1\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('BCC', 1) 464\n",
            "\t train 371\n",
            "\t test 93\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('BCC', 2) 35\n",
            "\t train 28\n",
            "\t test 7\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('BCC', 3) 9\n",
            "\t train 7\n",
            "\t test 2\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('BCC', 4) 1\n",
            "\t train 7\n",
            "\t test 0\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('BCC', 5) 2\n",
            "\t train 1\n",
            "\t test 1\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('BCC', 6) 3\n",
            "\t train 2\n",
            "\t test 1\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('BKL', 1) 991\n",
            "\t train 792\n",
            "\t test 199\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('BKL', 2) 55\n",
            "\t train 44\n",
            "\t test 11\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('BKL', 3) 12\n",
            "\t train 9\n",
            "\t test 3\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('BKL', 4) 13\n",
            "\t train 10\n",
            "\t test 3\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('BKL', 5) 7\n",
            "\t train 5\n",
            "\t test 2\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('BKL', 6) 21\n",
            "\t train 16\n",
            "\t test 5\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('DF', 1) 107\n",
            "\t train 85\n",
            "\t test 22\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('DF', 2) 3\n",
            "\t train 2\n",
            "\t test 1\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('DF', 3) 4\n",
            "\t train 3\n",
            "\t test 1\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('DF', 5) 1\n",
            "\t train 3\n",
            "\t test 0\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('MEL', 1) 993\n",
            "\t train 794\n",
            "\t test 199\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('MEL', 2) 53\n",
            "\t train 42\n",
            "\t test 11\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('MEL', 3) 31\n",
            "\t train 24\n",
            "\t test 7\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('MEL', 4) 8\n",
            "\t train 6\n",
            "\t test 2\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('MEL', 5) 9\n",
            "\t train 7\n",
            "\t test 2\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('MEL', 6) 19\n",
            "\t train 15\n",
            "\t test 4\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('NV', 1) 5021\n",
            "\t train 4016\n",
            "\t test 1005\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('NV', 2) 870\n",
            "\t train 696\n",
            "\t test 174\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('NV', 3) 451\n",
            "\t train 360\n",
            "\t test 91\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('NV', 4) 159\n",
            "\t train 127\n",
            "\t test 32\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('NV', 5) 87\n",
            "\t train 69\n",
            "\t test 18\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('NV', 6) 117\n",
            "\t train 93\n",
            "\t test 24\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('VASC', 1) 134\n",
            "\t train 107\n",
            "\t test 27\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('VASC', 2) 4\n",
            "\t train 3\n",
            "\t test 1\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('VASC', 3) 2\n",
            "\t train 1\n",
            "\t test 1\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('VASC', 5) 1\n",
            "\t train 1\n",
            "\t test 0\n",
            "\t val 0\n",
            "fizpatrick_skin_type: ('VASC', 6) 1\n",
            "\t train 1\n",
            "\t test 0\n",
            "\t val 0\n",
            "total_train: 7999 79.8701947079381\n",
            "total_test: 2024 20.20968547179231\n",
            "total_val: 2016 20.129805292061906\n",
            "Before augmentation\n",
            "AKIEC     3.25\n",
            "BCC       5.13\n",
            "BKL      10.95\n",
            "DF        1.14\n",
            "MEL      11.10\n",
            "NV       67.02\n",
            "VASC      1.41\n",
            "Name: label, dtype: float64\n",
            "/usr/local/lib/python3.8/dist-packages/imblearn/utils/_validation.py:587: FutureWarning: Pass sampling_strategy=not majority as keyword args. From version 0.9 passing these as positional arguments will result in an error\n",
            "  warnings.warn(\n",
            "   Unnamed: 0                                         image_path  \\\n",
            "0        7369  ISIC_2018/ISIC2018_Task3_Training_Input/ISIC_0...   \n",
            "1        7679  ISIC_2018/ISIC2018_Task3_Training_Input/ISIC_0...   \n",
            "2        7623  ISIC_2018/ISIC2018_Task3_Training_Input/ISIC_0...   \n",
            "3        2978  ISIC_2018/ISIC2018_Task3_Training_Input/ISIC_0...   \n",
            "4        1010  ISIC_2018/ISIC2018_Task3_Training_Input/ISIC_0...   \n",
            "\n",
            "                                   mask_path  fizpatrick_skin_type  \\\n",
            "0  ISIC_2018/masks/results1/ISIC_0027627.png                     1   \n",
            "1  ISIC_2018/masks/results1/ISIC_0026974.png                     1   \n",
            "2  ISIC_2018/masks/results1/ISIC_0032329.png                     1   \n",
            "3  ISIC_2018/masks/results1/ISIC_0025304.png                     1   \n",
            "4  ISIC_2018/masks/results1/ISIC_0033488.png                     1   \n",
            "\n",
            "       image_id MEL NV BCC  AKIEC BKL DF VASC  label  label_encoded  \\\n",
            "0  ISIC_0027627             AKIEC              AKIEC              0   \n",
            "1  ISIC_0026974             AKIEC              AKIEC              0   \n",
            "2  ISIC_0032329             AKIEC              AKIEC              0   \n",
            "3  ISIC_0025304             AKIEC              AKIEC              0   \n",
            "4  ISIC_0033488             AKIEC              AKIEC              0   \n",
            "\n",
            "          ita                                   transformed_path  \n",
            "0   87.579359  ISIC_2018/transformed/ISIC_2018/transformed/IS...  \n",
            "1  105.801370  ISIC_2018/transformed/ISIC_2018/transformed/IS...  \n",
            "2   74.216926  ISIC_2018/transformed/ISIC_2018/transformed/IS...  \n",
            "3   87.596405  ISIC_2018/transformed/ISIC_2018/transformed/IS...  \n",
            "4  106.593425  ISIC_2018/transformed/ISIC_2018/transformed/IS...  \n",
            "After augmentation\n",
            "AKIEC    5361\n",
            "BCC      5361\n",
            "BKL      5361\n",
            "DF       5361\n",
            "MEL      5361\n",
            "NV       5361\n",
            "VASC     5361\n",
            "Name: label, dtype: int64\n",
            "AKIEC    14.29\n",
            "BCC      14.29\n",
            "BKL      14.29\n",
            "DF       14.29\n",
            "MEL      14.29\n",
            "NV       14.29\n",
            "VASC     14.29\n",
            "Name: label, dtype: float64\n",
            "train size: 37527\n",
            "test size: 2024\n",
            "val size: 2016\n",
            "dataset sizes:41567. df size: 10015\n",
            "----\n",
            "AKIEC    5361\n",
            "BCC      5361\n",
            "BKL      5361\n",
            "DF       5361\n",
            "MEL      5361\n",
            "NV       5361\n",
            "VASC     5361\n",
            "Name: label, dtype: int64\n",
            "\t skin type 1 : 33384 (88.96)\n",
            "\t\tNV: 4016 (12.03)\n",
            "\t\tBKL: 4820 (14.44)\n",
            "\t\tMEL: 4803 (14.39)\n",
            "\t\tAKIEC: 4830 (14.47)\n",
            "\t\tBCC: 4850 (14.53)\n",
            "\t\tVASC: 5066 (15.17)\n",
            "\t\tDF: 4999 (14.97)\n",
            "\t skin type 2 : 2283 (6.08)\n",
            "\t\tNV: 696 (30.49)\n",
            "\t\tBKL: 284 (12.44)\n",
            "\t\tMEL: 228 (9.99)\n",
            "\t\tAKIEC: 435 (19.05)\n",
            "\t\tBCC: 369 (16.16)\n",
            "\t\tVASC: 143 (6.26)\n",
            "\t\tDF: 128 (5.61)\n",
            "\t skin type 3 : 935 (2.49)\n",
            "\t\tNV: 360 (38.50)\n",
            "\t\tBKL: 52 (5.56)\n",
            "\t\tMEL: 149 (15.94)\n",
            "\t\tAKIEC: 54 (5.78)\n",
            "\t\tBCC: 93 (9.95)\n",
            "\t\tVASC: 53 (5.67)\n",
            "\t\tDF: 174 (18.61)\n",
            "\t skin type 4 : 266 (0.71)\n",
            "\t\tNV: 127 (47.74)\n",
            "\t\tBKL: 74 (27.82)\n",
            "\t\tMEL: 37 (13.91)\n",
            "\t\tAKIEC: 16 (6.02)\n",
            "\t\tBCC: 12 (4.51)\n",
            "\t\tVASC: 0 (0.00)\n",
            "\t\tDF: 0 (0.00)\n",
            "\t skin type 5 : 270 (0.72)\n",
            "\t\tNV: 69 (25.56)\n",
            "\t\tBKL: 30 (11.11)\n",
            "\t\tMEL: 51 (18.89)\n",
            "\t\tAKIEC: 0 (0.00)\n",
            "\t\tBCC: 8 (2.96)\n",
            "\t\tVASC: 52 (19.26)\n",
            "\t\tDF: 60 (22.22)\n",
            "\t skin type 6 : 389 (1.04)\n",
            "\t\tNV: 93 (23.91)\n",
            "\t\tBKL: 101 (25.96)\n",
            "\t\tMEL: 93 (23.91)\n",
            "\t\tAKIEC: 26 (6.68)\n",
            "\t\tBCC: 29 (7.46)\n",
            "\t\tVASC: 47 (12.08)\n",
            "\t\tDF: 0 (0.00)\n",
            "----\n",
            "NV       1344\n",
            "MEL       225\n",
            "BKL       223\n",
            "BCC       106\n",
            "AKIEC      69\n",
            "VASC       31\n",
            "DF         26\n",
            "Name: label, dtype: int64\n",
            "\t skin type 1 : 1604 (79.25)\n",
            "\t\tNV: 1005 (62.66)\n",
            "\t\tBKL: 199 (12.41)\n",
            "\t\tMEL: 199 (12.41)\n",
            "\t\tAKIEC: 59 (3.68)\n",
            "\t\tBCC: 93 (5.80)\n",
            "\t\tVASC: 27 (1.68)\n",
            "\t\tDF: 22 (1.37)\n",
            "\t skin type 2 : 213 (10.52)\n",
            "\t\tNV: 174 (81.69)\n",
            "\t\tBKL: 11 (5.16)\n",
            "\t\tMEL: 11 (5.16)\n",
            "\t\tAKIEC: 6 (2.82)\n",
            "\t\tBCC: 7 (3.29)\n",
            "\t\tVASC: 2 (0.94)\n",
            "\t\tDF: 2 (0.94)\n",
            "\t skin type 3 : 109 (5.39)\n",
            "\t\tNV: 91 (83.49)\n",
            "\t\tBKL: 3 (2.75)\n",
            "\t\tMEL: 7 (6.42)\n",
            "\t\tAKIEC: 2 (1.83)\n",
            "\t\tBCC: 2 (1.83)\n",
            "\t\tVASC: 2 (1.83)\n",
            "\t\tDF: 2 (1.83)\n",
            "\t skin type 4 : 37 (1.83)\n",
            "\t\tNV: 32 (86.49)\n",
            "\t\tBKL: 3 (8.11)\n",
            "\t\tMEL: 2 (5.41)\n",
            "\t\tAKIEC: 0 (0.00)\n",
            "\t\tBCC: 0 (0.00)\n",
            "\t\tVASC: 0 (0.00)\n",
            "\t\tDF: 0 (0.00)\n",
            "\t skin type 5 : 24 (1.19)\n",
            "\t\tNV: 18 (75.00)\n",
            "\t\tBKL: 2 (8.33)\n",
            "\t\tMEL: 2 (8.33)\n",
            "\t\tAKIEC: 0 (0.00)\n",
            "\t\tBCC: 2 (8.33)\n",
            "\t\tVASC: 0 (0.00)\n",
            "\t\tDF: 0 (0.00)\n",
            "\t skin type 6 : 37 (1.83)\n",
            "\t\tNV: 24 (64.86)\n",
            "\t\tBKL: 5 (13.51)\n",
            "\t\tMEL: 4 (10.81)\n",
            "\t\tAKIEC: 2 (5.41)\n",
            "\t\tBCC: 2 (5.41)\n",
            "\t\tVASC: 0 (0.00)\n",
            "\t\tDF: 0 (0.00)\n",
            "----\n",
            "NV       1344\n",
            "MEL       225\n",
            "BKL       223\n",
            "BCC       104\n",
            "AKIEC      67\n",
            "VASC       29\n",
            "DF         24\n",
            "Name: label, dtype: int64\n",
            "\t skin type 1 : 1604 (79.56)\n",
            "\t\tNV: 1005 (62.66)\n",
            "\t\tBKL: 199 (12.41)\n",
            "\t\tMEL: 199 (12.41)\n",
            "\t\tAKIEC: 59 (3.68)\n",
            "\t\tBCC: 93 (5.80)\n",
            "\t\tVASC: 27 (1.68)\n",
            "\t\tDF: 22 (1.37)\n",
            "\t skin type 2 : 211 (10.47)\n",
            "\t\tNV: 174 (82.46)\n",
            "\t\tBKL: 11 (5.21)\n",
            "\t\tMEL: 11 (5.21)\n",
            "\t\tAKIEC: 6 (2.84)\n",
            "\t\tBCC: 7 (3.32)\n",
            "\t\tVASC: 1 (0.47)\n",
            "\t\tDF: 1 (0.47)\n",
            "\t skin type 3 : 106 (5.26)\n",
            "\t\tNV: 91 (85.85)\n",
            "\t\tBKL: 3 (2.83)\n",
            "\t\tMEL: 7 (6.60)\n",
            "\t\tAKIEC: 1 (0.94)\n",
            "\t\tBCC: 2 (1.89)\n",
            "\t\tVASC: 1 (0.94)\n",
            "\t\tDF: 1 (0.94)\n",
            "\t skin type 4 : 37 (1.84)\n",
            "\t\tNV: 32 (86.49)\n",
            "\t\tBKL: 3 (8.11)\n",
            "\t\tMEL: 2 (5.41)\n",
            "\t\tAKIEC: 0 (0.00)\n",
            "\t\tBCC: 0 (0.00)\n",
            "\t\tVASC: 0 (0.00)\n",
            "\t\tDF: 0 (0.00)\n",
            "\t skin type 5 : 23 (1.14)\n",
            "\t\tNV: 18 (78.26)\n",
            "\t\tBKL: 2 (8.70)\n",
            "\t\tMEL: 2 (8.70)\n",
            "\t\tAKIEC: 0 (0.00)\n",
            "\t\tBCC: 1 (4.35)\n",
            "\t\tVASC: 0 (0.00)\n",
            "\t\tDF: 0 (0.00)\n",
            "\t skin type 6 : 35 (1.74)\n",
            "\t\tNV: 24 (68.57)\n",
            "\t\tBKL: 5 (14.29)\n",
            "\t\tMEL: 4 (11.43)\n",
            "\t\tAKIEC: 1 (2.86)\n",
            "\t\tBCC: 1 (2.86)\n",
            "\t\tVASC: 0 (0.00)\n",
            "\t\tDF: 0 (0.00)\n",
            "train size: 37527\n",
            "val size: 2016\n",
            "train skin types: [1 2 3 4 6 5]\n",
            "val skin types: [1 2 3 6 5 4]\n",
            "train skin conditions: 7\n",
            "val skin conditions: 7\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNeXt101_32X8D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt101_32X8D_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\" to /root/.cache/torch/hub/checkpoints/resnext101_32x8d-8ba56ff5.pth\n",
            "100% 340M/340M [00:03<00:00, 118MB/s]\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 64, 64]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 64, 64]             128\n",
            "              ReLU-3           [-1, 64, 64, 64]               0\n",
            "         MaxPool2d-4           [-1, 64, 32, 32]               0\n",
            "            Conv2d-5          [-1, 256, 32, 32]          16,384\n",
            "       BatchNorm2d-6          [-1, 256, 32, 32]             512\n",
            "              ReLU-7          [-1, 256, 32, 32]               0\n",
            "            Conv2d-8          [-1, 256, 32, 32]          18,432\n",
            "       BatchNorm2d-9          [-1, 256, 32, 32]             512\n",
            "             ReLU-10          [-1, 256, 32, 32]               0\n",
            "           Conv2d-11          [-1, 256, 32, 32]          65,536\n",
            "      BatchNorm2d-12          [-1, 256, 32, 32]             512\n",
            "           Conv2d-13          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 32, 32]             512\n",
            "             ReLU-15          [-1, 256, 32, 32]               0\n",
            "       Bottleneck-16          [-1, 256, 32, 32]               0\n",
            "           Conv2d-17          [-1, 256, 32, 32]          65,536\n",
            "      BatchNorm2d-18          [-1, 256, 32, 32]             512\n",
            "             ReLU-19          [-1, 256, 32, 32]               0\n",
            "           Conv2d-20          [-1, 256, 32, 32]          18,432\n",
            "      BatchNorm2d-21          [-1, 256, 32, 32]             512\n",
            "             ReLU-22          [-1, 256, 32, 32]               0\n",
            "           Conv2d-23          [-1, 256, 32, 32]          65,536\n",
            "      BatchNorm2d-24          [-1, 256, 32, 32]             512\n",
            "             ReLU-25          [-1, 256, 32, 32]               0\n",
            "       Bottleneck-26          [-1, 256, 32, 32]               0\n",
            "           Conv2d-27          [-1, 256, 32, 32]          65,536\n",
            "      BatchNorm2d-28          [-1, 256, 32, 32]             512\n",
            "             ReLU-29          [-1, 256, 32, 32]               0\n",
            "           Conv2d-30          [-1, 256, 32, 32]          18,432\n",
            "      BatchNorm2d-31          [-1, 256, 32, 32]             512\n",
            "             ReLU-32          [-1, 256, 32, 32]               0\n",
            "           Conv2d-33          [-1, 256, 32, 32]          65,536\n",
            "      BatchNorm2d-34          [-1, 256, 32, 32]             512\n",
            "             ReLU-35          [-1, 256, 32, 32]               0\n",
            "       Bottleneck-36          [-1, 256, 32, 32]               0\n",
            "           Conv2d-37          [-1, 512, 32, 32]         131,072\n",
            "      BatchNorm2d-38          [-1, 512, 32, 32]           1,024\n",
            "             ReLU-39          [-1, 512, 32, 32]               0\n",
            "           Conv2d-40          [-1, 512, 16, 16]          73,728\n",
            "      BatchNorm2d-41          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-42          [-1, 512, 16, 16]               0\n",
            "           Conv2d-43          [-1, 512, 16, 16]         262,144\n",
            "      BatchNorm2d-44          [-1, 512, 16, 16]           1,024\n",
            "           Conv2d-45          [-1, 512, 16, 16]         131,072\n",
            "      BatchNorm2d-46          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-47          [-1, 512, 16, 16]               0\n",
            "       Bottleneck-48          [-1, 512, 16, 16]               0\n",
            "           Conv2d-49          [-1, 512, 16, 16]         262,144\n",
            "      BatchNorm2d-50          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-51          [-1, 512, 16, 16]               0\n",
            "           Conv2d-52          [-1, 512, 16, 16]          73,728\n",
            "      BatchNorm2d-53          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-54          [-1, 512, 16, 16]               0\n",
            "           Conv2d-55          [-1, 512, 16, 16]         262,144\n",
            "      BatchNorm2d-56          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-57          [-1, 512, 16, 16]               0\n",
            "       Bottleneck-58          [-1, 512, 16, 16]               0\n",
            "           Conv2d-59          [-1, 512, 16, 16]         262,144\n",
            "      BatchNorm2d-60          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-61          [-1, 512, 16, 16]               0\n",
            "           Conv2d-62          [-1, 512, 16, 16]          73,728\n",
            "      BatchNorm2d-63          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-64          [-1, 512, 16, 16]               0\n",
            "           Conv2d-65          [-1, 512, 16, 16]         262,144\n",
            "      BatchNorm2d-66          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-67          [-1, 512, 16, 16]               0\n",
            "       Bottleneck-68          [-1, 512, 16, 16]               0\n",
            "           Conv2d-69          [-1, 512, 16, 16]         262,144\n",
            "      BatchNorm2d-70          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-71          [-1, 512, 16, 16]               0\n",
            "           Conv2d-72          [-1, 512, 16, 16]          73,728\n",
            "      BatchNorm2d-73          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-74          [-1, 512, 16, 16]               0\n",
            "           Conv2d-75          [-1, 512, 16, 16]         262,144\n",
            "      BatchNorm2d-76          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-77          [-1, 512, 16, 16]               0\n",
            "       Bottleneck-78          [-1, 512, 16, 16]               0\n",
            "           Conv2d-79         [-1, 1024, 16, 16]         524,288\n",
            "      BatchNorm2d-80         [-1, 1024, 16, 16]           2,048\n",
            "             ReLU-81         [-1, 1024, 16, 16]               0\n",
            "           Conv2d-82           [-1, 1024, 8, 8]         294,912\n",
            "      BatchNorm2d-83           [-1, 1024, 8, 8]           2,048\n",
            "             ReLU-84           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-85           [-1, 1024, 8, 8]       1,048,576\n",
            "      BatchNorm2d-86           [-1, 1024, 8, 8]           2,048\n",
            "           Conv2d-87           [-1, 1024, 8, 8]         524,288\n",
            "      BatchNorm2d-88           [-1, 1024, 8, 8]           2,048\n",
            "             ReLU-89           [-1, 1024, 8, 8]               0\n",
            "       Bottleneck-90           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-91           [-1, 1024, 8, 8]       1,048,576\n",
            "      BatchNorm2d-92           [-1, 1024, 8, 8]           2,048\n",
            "             ReLU-93           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-94           [-1, 1024, 8, 8]         294,912\n",
            "      BatchNorm2d-95           [-1, 1024, 8, 8]           2,048\n",
            "             ReLU-96           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-97           [-1, 1024, 8, 8]       1,048,576\n",
            "      BatchNorm2d-98           [-1, 1024, 8, 8]           2,048\n",
            "             ReLU-99           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-100           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-101           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-102           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-103           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-104           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-105           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-106           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-107           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-108           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-109           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-110           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-111           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-112           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-113           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-114           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-115           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-116           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-117           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-118           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-119           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-120           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-121           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-122           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-123           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-124           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-125           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-126           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-127           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-128           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-129           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-130           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-131           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-132           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-133           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-134           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-135           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-136           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-137           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-138           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-139           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-140           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-141           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-142           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-143           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-144           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-145           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-146           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-147           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-148           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-149           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-150           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-151           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-152           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-153           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-154           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-155           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-156           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-157           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-158           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-159           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-160           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-161           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-162           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-163           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-164           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-165           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-166           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-167           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-168           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-169           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-170           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-171           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-172           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-173           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-174           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-175           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-176           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-177           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-178           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-179           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-180           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-181           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-182           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-183           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-184           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-185           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-186           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-187           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-188           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-189           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-190           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-191           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-192           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-193           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-194           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-195           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-196           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-197           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-198           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-199           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-200           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-201           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-202           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-203           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-204           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-205           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-206           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-207           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-208           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-209           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-210           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-211           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-212           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-213           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-214           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-215           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-216           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-217           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-218           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-219           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-220           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-221           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-222           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-223           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-224           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-225           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-226           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-227           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-228           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-229           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-230           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-231           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-232           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-233           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-234           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-235           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-236           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-237           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-238           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-239           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-240           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-241           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-242           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-243           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-244           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-245           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-246           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-247           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-248           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-249           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-250           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-251           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-252           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-253           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-254           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-255           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-256           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-257           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-258           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-259           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-260           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-261           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-262           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-263           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-264           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-265           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-266           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-267           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-268           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-269           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-270           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-271           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-272           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-273           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-274           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-275           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-276           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-277           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-278           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-279           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-280           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-281           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-282           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-283           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-284           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-285           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-286           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-287           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-288           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-289           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-290           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-291           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-292           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-293           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-294           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-295           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-296           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-297           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-298           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-299           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-300           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-301           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-302           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-303           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-304           [-1, 1024, 8, 8]         294,912\n",
            "     BatchNorm2d-305           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-306           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-307           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-308           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-309           [-1, 1024, 8, 8]               0\n",
            "      Bottleneck-310           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-311           [-1, 2048, 8, 8]       2,097,152\n",
            "     BatchNorm2d-312           [-1, 2048, 8, 8]           4,096\n",
            "            ReLU-313           [-1, 2048, 8, 8]               0\n",
            "          Conv2d-314           [-1, 2048, 4, 4]       1,179,648\n",
            "     BatchNorm2d-315           [-1, 2048, 4, 4]           4,096\n",
            "            ReLU-316           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-317           [-1, 2048, 4, 4]       4,194,304\n",
            "     BatchNorm2d-318           [-1, 2048, 4, 4]           4,096\n",
            "          Conv2d-319           [-1, 2048, 4, 4]       2,097,152\n",
            "     BatchNorm2d-320           [-1, 2048, 4, 4]           4,096\n",
            "            ReLU-321           [-1, 2048, 4, 4]               0\n",
            "      Bottleneck-322           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-323           [-1, 2048, 4, 4]       4,194,304\n",
            "     BatchNorm2d-324           [-1, 2048, 4, 4]           4,096\n",
            "            ReLU-325           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-326           [-1, 2048, 4, 4]       1,179,648\n",
            "     BatchNorm2d-327           [-1, 2048, 4, 4]           4,096\n",
            "            ReLU-328           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-329           [-1, 2048, 4, 4]       4,194,304\n",
            "     BatchNorm2d-330           [-1, 2048, 4, 4]           4,096\n",
            "            ReLU-331           [-1, 2048, 4, 4]               0\n",
            "      Bottleneck-332           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-333           [-1, 2048, 4, 4]       4,194,304\n",
            "     BatchNorm2d-334           [-1, 2048, 4, 4]           4,096\n",
            "            ReLU-335           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-336           [-1, 2048, 4, 4]       1,179,648\n",
            "     BatchNorm2d-337           [-1, 2048, 4, 4]           4,096\n",
            "            ReLU-338           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-339           [-1, 2048, 4, 4]       4,194,304\n",
            "     BatchNorm2d-340           [-1, 2048, 4, 4]           4,096\n",
            "            ReLU-341           [-1, 2048, 4, 4]               0\n",
            "      Bottleneck-342           [-1, 2048, 4, 4]               0\n",
            "AdaptiveAvgPool2d-343           [-1, 2048, 1, 1]               0\n",
            "          Linear-344                    [-1, 7]          14,343\n",
            "          ResNet-345                    [-1, 7]               0\n",
            "================================================================\n",
            "Total params: 86,756,679\n",
            "Trainable params: 86,756,679\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.19\n",
            "Forward/backward pass size (MB): 252.27\n",
            "Params size (MB): 330.95\n",
            "Estimated Total Size (MB): 583.40\n",
            "----------------------------------------------------------------\n",
            "Epoch 0: Best val loss inf, Best val acc 0.0000, best val recall 0.0000, best val precision 0.0000\n",
            "  4%|█▍                                  | 48/1172 [00:27<10:16,  1.82it/s]\n",
            "[epoch 0], [iter 50 / 1172], [train loss 1.6975], [train acc 0.5631]\n",
            "  8%|███                                 | 98/1172 [00:54<09:55,  1.80it/s]\n",
            "[epoch 0], [iter 100 / 1172], [train loss 2.3476], [train acc 0.4754]\n",
            " 13%|████▍                              | 148/1172 [01:21<09:04,  1.88it/s]\n",
            "[epoch 0], [iter 150 / 1172], [train loss 1.5692], [train acc 0.6514]\n",
            " 17%|█████▉                             | 198/1172 [01:48<08:33,  1.90it/s]\n",
            "[epoch 0], [iter 200 / 1172], [train loss 1.1749], [train acc 0.7390]\n",
            " 21%|███████▍                           | 248/1172 [02:15<08:11,  1.88it/s]\n",
            "[epoch 0], [iter 250 / 1172], [train loss 1.1029], [train acc 0.7811]\n",
            " 25%|████████▉                          | 298/1172 [02:41<07:28,  1.95it/s]\n",
            "[epoch 0], [iter 300 / 1172], [train loss 1.0938], [train acc 0.7746]\n",
            " 30%|██████████▍                        | 348/1172 [03:07<07:03,  1.95it/s]\n",
            "[epoch 0], [iter 350 / 1172], [train loss 0.9371], [train acc 0.8069]\n",
            " 34%|███████████▉                       | 398/1172 [03:33<06:43,  1.92it/s]\n",
            "[epoch 0], [iter 400 / 1172], [train loss 0.8197], [train acc 0.8311]\n",
            " 38%|█████████████▍                     | 448/1172 [03:59<06:13,  1.94it/s]\n",
            "[epoch 0], [iter 450 / 1172], [train loss 0.8554], [train acc 0.8244]\n",
            " 42%|██████████████▊                    | 498/1172 [04:24<05:46,  1.94it/s]\n",
            "[epoch 0], [iter 500 / 1172], [train loss 0.7699], [train acc 0.8420]\n",
            " 47%|████████████████▎                  | 548/1172 [04:50<05:21,  1.94it/s]\n",
            "[epoch 0], [iter 550 / 1172], [train loss 0.6998], [train acc 0.8564]\n",
            " 51%|█████████████████▊                 | 598/1172 [05:16<04:57,  1.93it/s]\n",
            "[epoch 0], [iter 600 / 1172], [train loss 0.7558], [train acc 0.8454]\n",
            " 55%|███████████████████▎               | 648/1172 [05:42<04:29,  1.94it/s]\n",
            "[epoch 0], [iter 650 / 1172], [train loss 0.6986], [train acc 0.8573]\n",
            " 60%|████████████████████▊              | 698/1172 [06:08<04:03,  1.95it/s]\n",
            "[epoch 0], [iter 700 / 1172], [train loss 0.6489], [train acc 0.8675]\n",
            " 64%|██████████████████████▎            | 748/1172 [06:34<03:42,  1.90it/s]\n",
            "[epoch 0], [iter 750 / 1172], [train loss 0.7071], [train acc 0.8569]\n",
            " 68%|███████████████████████▊           | 798/1172 [06:59<03:11,  1.95it/s]\n",
            "[epoch 0], [iter 800 / 1172], [train loss 0.6633], [train acc 0.8658]\n",
            " 72%|█████████████████████████▎         | 848/1172 [07:25<02:47,  1.94it/s]\n",
            "[epoch 0], [iter 850 / 1172], [train loss 0.6244], [train acc 0.8737]\n",
            " 77%|██████████████████████████▊        | 898/1172 [07:51<02:20,  1.96it/s]\n",
            "[epoch 0], [iter 900 / 1172], [train loss 0.6940], [train acc 0.8597]\n",
            " 81%|████████████████████████████▎      | 948/1172 [08:17<01:55,  1.93it/s]\n",
            "[epoch 0], [iter 950 / 1172], [train loss 0.6600], [train acc 0.8671]\n",
            " 85%|█████████████████████████████▊     | 998/1172 [08:43<01:29,  1.94it/s]\n",
            "[epoch 0], [iter 1000 / 1172], [train loss 0.6270], [train acc 0.8738]\n",
            " 89%|██████████████████████████████▍   | 1048/1172 [09:09<01:04,  1.93it/s]\n",
            "[epoch 0], [iter 1050 / 1172], [train loss 0.6721], [train acc 0.8654]\n",
            " 94%|███████████████████████████████▊  | 1098/1172 [09:34<00:38,  1.94it/s]\n",
            "[epoch 0], [iter 1100 / 1172], [train loss 0.6416], [train acc 0.8715]\n",
            " 98%|█████████████████████████████████▎| 1148/1172 [10:00<00:12,  1.92it/s]\n",
            "[epoch 0], [iter 1150 / 1172], [train loss 0.6138], [train acc 0.8771]\n",
            "\t>>> Training: Loss 0.6017, Reg 0.000, Acc 0.8795, precision: 0.8795, recall0.8795\n",
            "\t>>> Val.    : Loss 12.6294, Reg 0.000, Acc 0.0144, precision: 0.0144, recall0.0144\n",
            "Saved model with highest acc ...\n",
            "Epoch 1: Best val loss 12.6294, Best val acc 0.0144, best val recall 0.0144, best val precision 0.0144\n",
            "  4%|█▍                                  | 48/1172 [00:25<09:43,  1.93it/s]\n",
            "[epoch 1], [iter 50 / 1172], [train loss 0.7038], [train acc 0.8526]\n",
            "  8%|███                                 | 98/1172 [00:51<09:19,  1.92it/s]\n",
            "[epoch 1], [iter 100 / 1172], [train loss 0.7670], [train acc 0.8262]\n",
            " 13%|████▍                              | 148/1172 [01:17<08:59,  1.90it/s]\n",
            "[epoch 1], [iter 150 / 1172], [train loss 0.7541], [train acc 0.8299]\n",
            " 17%|█████▉                             | 198/1172 [01:44<08:48,  1.84it/s]\n",
            "[epoch 1], [iter 200 / 1172], [train loss 0.7267], [train acc 0.8361]\n",
            " 21%|███████▍                           | 248/1172 [02:12<08:19,  1.85it/s]\n",
            "[epoch 1], [iter 250 / 1172], [train loss 0.7184], [train acc 0.8400]\n",
            " 25%|████████▉                          | 298/1172 [02:38<07:26,  1.96it/s]\n",
            "[epoch 1], [iter 300 / 1172], [train loss 0.7401], [train acc 0.8334]\n",
            " 30%|██████████▍                        | 348/1172 [03:04<07:04,  1.94it/s]\n",
            "[epoch 1], [iter 350 / 1172], [train loss 0.7159], [train acc 0.8389]\n",
            " 34%|███████████▉                       | 398/1172 [03:29<06:39,  1.94it/s]\n",
            "[epoch 1], [iter 400 / 1172], [train loss 0.6932], [train acc 0.8440]\n",
            " 38%|█████████████▍                     | 448/1172 [03:55<06:20,  1.90it/s]\n",
            "[epoch 1], [iter 450 / 1172], [train loss 0.7074], [train acc 0.8411]\n",
            " 42%|██████████████▊                    | 498/1172 [04:21<05:44,  1.96it/s]\n",
            "[epoch 1], [iter 500 / 1172], [train loss 0.6863], [train acc 0.8458]\n",
            " 47%|████████████████▎                  | 548/1172 [04:47<05:24,  1.92it/s]\n",
            "[epoch 1], [iter 550 / 1172], [train loss 0.6664], [train acc 0.8503]\n",
            " 51%|█████████████████▊                 | 598/1172 [05:12<04:53,  1.96it/s]\n",
            "[epoch 1], [iter 600 / 1172], [train loss 0.6748], [train acc 0.8489]\n",
            " 55%|███████████████████▎               | 648/1172 [05:38<04:28,  1.95it/s]\n",
            "[epoch 1], [iter 650 / 1172], [train loss 0.6563], [train acc 0.8531]\n",
            " 60%|████████████████████▊              | 698/1172 [06:04<04:03,  1.95it/s]\n",
            "[epoch 1], [iter 700 / 1172], [train loss 0.6388], [train acc 0.8570]\n",
            " 64%|██████████████████████▎            | 748/1172 [06:29<03:37,  1.95it/s]\n",
            "[epoch 1], [iter 750 / 1172], [train loss 0.6501], [train acc 0.8556]\n",
            " 68%|███████████████████████▊           | 798/1172 [06:55<03:12,  1.95it/s]\n",
            "[epoch 1], [iter 800 / 1172], [train loss 0.6338], [train acc 0.8592]\n",
            " 72%|█████████████████████████▎         | 848/1172 [07:21<02:46,  1.94it/s]\n",
            "[epoch 1], [iter 850 / 1172], [train loss 0.6181], [train acc 0.8627]\n",
            " 77%|██████████████████████████▊        | 898/1172 [07:47<02:21,  1.93it/s]\n",
            "[epoch 1], [iter 900 / 1172], [train loss 0.6397], [train acc 0.8587]\n",
            " 81%|████████████████████████████▎      | 948/1172 [08:12<01:54,  1.96it/s]\n",
            "[epoch 1], [iter 950 / 1172], [train loss 0.6277], [train acc 0.8621]\n",
            " 85%|█████████████████████████████▊     | 998/1172 [08:38<01:28,  1.97it/s]\n",
            "[epoch 1], [iter 1000 / 1172], [train loss 0.6134], [train acc 0.8653]\n",
            " 89%|██████████████████████████████▍   | 1048/1172 [09:04<01:04,  1.92it/s]\n",
            "[epoch 1], [iter 1050 / 1172], [train loss 0.6392], [train acc 0.8581]\n",
            " 94%|███████████████████████████████▊  | 1098/1172 [09:29<00:38,  1.93it/s]\n",
            "[epoch 1], [iter 1100 / 1172], [train loss 0.6255], [train acc 0.8612]\n",
            " 98%|█████████████████████████████████▎| 1148/1172 [09:55<00:12,  1.94it/s]\n",
            "[epoch 1], [iter 1150 / 1172], [train loss 0.6121], [train acc 0.8642]\n",
            "\t>>> Training: Loss 0.6061, Reg 0.000, Acc 0.8655, precision: 0.8655, recall0.8655\n",
            "\t>>> Val.    : Loss 10.5636, Reg 0.000, Acc 0.0144, precision: 0.0144, recall0.0144\n",
            "Epoch 2: Best val loss 10.5636, Best val acc 0.0144, best val recall 0.0144, best val precision 0.0144\n",
            "  4%|█▍                                  | 48/1172 [00:25<09:39,  1.94it/s]\n",
            "[epoch 2], [iter 50 / 1172], [train loss 0.6492], [train acc 0.8503]\n",
            "  8%|███                                 | 98/1172 [00:51<09:21,  1.91it/s]\n",
            "[epoch 2], [iter 100 / 1172], [train loss 0.6814], [train acc 0.8346]\n",
            " 13%|████▍                              | 148/1172 [01:17<08:49,  1.93it/s]\n",
            "[epoch 2], [iter 150 / 1172], [train loss 0.6869], [train acc 0.8314]\n",
            " 17%|█████▉                             | 198/1172 [01:43<08:21,  1.94it/s]\n",
            "[epoch 2], [iter 200 / 1172], [train loss 0.6734], [train acc 0.8347]\n",
            " 21%|███████▍                           | 248/1172 [02:09<08:05,  1.90it/s]\n",
            "[epoch 2], [iter 250 / 1172], [train loss 0.6701], [train acc 0.8369]\n",
            " 25%|████████▉                          | 298/1172 [02:35<07:30,  1.94it/s]\n",
            "[epoch 2], [iter 300 / 1172], [train loss 0.6727], [train acc 0.8364]\n",
            " 30%|██████████▍                        | 348/1172 [03:01<07:02,  1.95it/s]\n",
            "[epoch 2], [iter 350 / 1172], [train loss 0.6602], [train acc 0.8394]\n",
            " 34%|███████████▉                       | 398/1172 [03:26<06:37,  1.94it/s]\n",
            "[epoch 2], [iter 400 / 1172], [train loss 0.6482], [train acc 0.8424]\n",
            " 38%|█████████████▍                     | 448/1172 [03:52<06:10,  1.95it/s]\n",
            "[epoch 2], [iter 450 / 1172], [train loss 0.6519], [train acc 0.8425]\n",
            " 42%|██████████████▊                    | 498/1172 [04:18<05:56,  1.89it/s]\n",
            "[epoch 2], [iter 500 / 1172], [train loss 0.6405], [train acc 0.8452]\n",
            " 47%|████████████████▎                  | 548/1172 [04:43<05:28,  1.90it/s]\n",
            "[epoch 2], [iter 550 / 1172], [train loss 0.6294], [train acc 0.8479]\n",
            " 51%|█████████████████▊                 | 598/1172 [05:09<04:54,  1.95it/s]\n",
            "[epoch 2], [iter 600 / 1172], [train loss 0.6335], [train acc 0.8477]\n",
            " 55%|███████████████████▎               | 648/1172 [05:35<04:26,  1.97it/s]\n",
            "[epoch 2], [iter 650 / 1172], [train loss 0.6230], [train acc 0.8503]\n",
            " 60%|████████████████████▊              | 698/1172 [06:00<04:06,  1.92it/s]\n",
            "[epoch 2], [iter 700 / 1172], [train loss 0.6127], [train acc 0.8527]\n",
            " 64%|██████████████████████▎            | 748/1172 [06:26<03:39,  1.93it/s]\n",
            "[epoch 2], [iter 750 / 1172], [train loss 0.6237], [train acc 0.8513]\n",
            " 68%|███████████████████████▊           | 798/1172 [06:52<03:10,  1.96it/s]\n",
            "[epoch 2], [iter 800 / 1172], [train loss 0.6138], [train acc 0.8536]\n",
            " 72%|█████████████████████████▎         | 848/1172 [07:18<02:45,  1.96it/s]\n",
            "[epoch 2], [iter 850 / 1172], [train loss 0.6043], [train acc 0.8559]\n",
            " 77%|██████████████████████████▊        | 898/1172 [07:43<02:21,  1.94it/s]\n",
            "[epoch 2], [iter 900 / 1172], [train loss 0.6216], [train acc 0.8530]\n",
            " 81%|████████████████████████████▎      | 948/1172 [08:09<01:54,  1.96it/s]\n",
            "[epoch 2], [iter 950 / 1172], [train loss 0.6143], [train acc 0.8552]\n",
            " 85%|█████████████████████████████▊     | 998/1172 [08:35<01:29,  1.95it/s]\n",
            "[epoch 2], [iter 1000 / 1172], [train loss 0.6052], [train acc 0.8574]\n",
            " 89%|██████████████████████████████▍   | 1048/1172 [09:01<01:03,  1.94it/s]\n",
            "[epoch 2], [iter 1050 / 1172], [train loss 0.6327], [train acc 0.8497]\n",
            " 94%|███████████████████████████████▊  | 1098/1172 [09:26<00:37,  1.95it/s]\n",
            "[epoch 2], [iter 1100 / 1172], [train loss 0.6240], [train acc 0.8518]\n",
            " 98%|█████████████████████████████████▎| 1148/1172 [09:52<00:12,  1.96it/s]\n",
            "[epoch 2], [iter 1150 / 1172], [train loss 0.6151], [train acc 0.8540]\n",
            "\t>>> Training: Loss 0.6111, Reg 0.000, Acc 0.8549, precision: 0.8549, recall0.8549\n",
            "\t>>> Val.    : Loss 13.0632, Reg 0.000, Acc 0.0144, precision: 0.0144, recall0.0144\n",
            "Epoch 3: Best val loss 10.5636, Best val acc 0.0144, best val recall 0.0144, best val precision 0.0144\n",
            "  4%|█▍                                  | 48/1172 [00:25<09:42,  1.93it/s]\n",
            "[epoch 3], [iter 50 / 1172], [train loss 0.6373], [train acc 0.8476]\n",
            "  8%|███                                 | 98/1172 [00:51<09:07,  1.96it/s]\n",
            "[epoch 3], [iter 100 / 1172], [train loss 0.6622], [train acc 0.8401]\n",
            " 13%|████▍                              | 148/1172 [01:17<08:44,  1.95it/s]\n",
            "[epoch 3], [iter 150 / 1172], [train loss 0.6597], [train acc 0.8407]\n",
            " 17%|█████▉                             | 198/1172 [01:42<08:19,  1.95it/s]\n",
            "[epoch 3], [iter 200 / 1172], [train loss 0.6508], [train acc 0.8429]\n",
            " 21%|███████▍                           | 248/1172 [02:08<07:53,  1.95it/s]\n",
            "[epoch 3], [iter 250 / 1172], [train loss 0.6495], [train acc 0.8443]\n",
            " 25%|████████▉                          | 298/1172 [02:34<07:34,  1.92it/s]\n",
            "[epoch 3], [iter 300 / 1172], [train loss 0.6538], [train acc 0.8430]\n",
            " 30%|██████████▍                        | 348/1172 [03:00<07:03,  1.95it/s]\n",
            "[epoch 3], [iter 350 / 1172], [train loss 0.6454], [train acc 0.8450]\n",
            " 34%|███████████▉                       | 398/1172 [03:25<06:35,  1.96it/s]\n",
            "[epoch 3], [iter 400 / 1172], [train loss 0.6371], [train acc 0.8470]\n",
            " 38%|█████████████▍                     | 448/1172 [03:51<06:10,  1.95it/s]\n",
            "[epoch 3], [iter 450 / 1172], [train loss 0.6438], [train acc 0.8456]\n",
            " 42%|██████████████▊                    | 498/1172 [04:17<05:47,  1.94it/s]\n",
            "[epoch 3], [iter 500 / 1172], [train loss 0.6357], [train acc 0.8475]\n",
            " 47%|████████████████▎                  | 548/1172 [04:42<05:20,  1.95it/s]\n",
            "[epoch 3], [iter 550 / 1172], [train loss 0.6279], [train acc 0.8494]\n",
            " 51%|█████████████████▊                 | 598/1172 [05:08<04:54,  1.95it/s]\n",
            "[epoch 3], [iter 600 / 1172], [train loss 0.6340], [train acc 0.8484]\n",
            " 55%|███████████████████▎               | 648/1172 [05:34<04:28,  1.95it/s]\n",
            "[epoch 3], [iter 650 / 1172], [train loss 0.6264], [train acc 0.8502]\n",
            " 60%|████████████████████▊              | 698/1172 [05:59<04:02,  1.96it/s]\n",
            "[epoch 3], [iter 700 / 1172], [train loss 0.6189], [train acc 0.8520]\n",
            " 64%|██████████████████████▎            | 748/1172 [06:25<03:36,  1.95it/s]\n",
            "[epoch 3], [iter 750 / 1172], [train loss 0.6211], [train acc 0.8518]\n",
            " 68%|███████████████████████▊           | 798/1172 [06:50<03:11,  1.95it/s]\n",
            "[epoch 3], [iter 800 / 1172], [train loss 0.6139], [train acc 0.8536]\n",
            " 72%|█████████████████████████▎         | 848/1172 [07:16<02:46,  1.94it/s]\n",
            "[epoch 3], [iter 850 / 1172], [train loss 0.6069], [train acc 0.8552]\n",
            " 77%|██████████████████████████▊        | 898/1172 [07:42<02:21,  1.93it/s]\n",
            "[epoch 3], [iter 900 / 1172], [train loss 0.6129], [train acc 0.8548]\n",
            " 81%|████████████████████████████▎      | 948/1172 [08:08<01:54,  1.96it/s]\n",
            "[epoch 3], [iter 950 / 1172], [train loss 0.6063], [train acc 0.8564]\n",
            " 85%|█████████████████████████████▊     | 998/1172 [08:33<01:28,  1.96it/s]\n",
            "[epoch 3], [iter 1000 / 1172], [train loss 0.5996], [train acc 0.8580]\n",
            " 89%|██████████████████████████████▍   | 1048/1172 [08:59<01:05,  1.89it/s]\n",
            "[epoch 3], [iter 1050 / 1172], [train loss 0.6144], [train acc 0.8540]\n",
            " 94%|███████████████████████████████▊  | 1098/1172 [09:25<00:37,  1.95it/s]\n",
            "[epoch 3], [iter 1100 / 1172], [train loss 0.6094], [train acc 0.8555]\n",
            " 98%|█████████████████████████████████▎| 1148/1172 [09:50<00:12,  1.96it/s]\n",
            "[epoch 3], [iter 1150 / 1172], [train loss 0.6029], [train acc 0.8571]\n",
            "\t>>> Training: Loss 0.5999, Reg 0.000, Acc 0.8578, precision: 0.8578, recall0.8578\n",
            "\t>>> Val.    : Loss 12.1969, Reg 0.000, Acc 0.0144, precision: 0.0144, recall0.0144\n",
            "Epoch 4: Best val loss 10.5636, Best val acc 0.0144, best val recall 0.0144, best val precision 0.0144\n",
            "  4%|█▍                                  | 48/1172 [00:25<09:43,  1.93it/s]\n",
            "[epoch 4], [iter 50 / 1172], [train loss 0.6266], [train acc 0.8498]\n",
            "  8%|███                                 | 98/1172 [00:51<09:10,  1.95it/s]\n",
            "[epoch 4], [iter 100 / 1172], [train loss 0.6414], [train acc 0.8431]\n",
            " 13%|████▍                              | 148/1172 [01:17<08:45,  1.95it/s]\n",
            "[epoch 4], [iter 150 / 1172], [train loss 0.6389], [train acc 0.8440]\n",
            " 17%|█████▉                             | 198/1172 [01:43<08:22,  1.94it/s]\n",
            "[epoch 4], [iter 200 / 1172], [train loss 0.6324], [train acc 0.8455]\n",
            " 21%|███████▍                           | 248/1172 [02:08<08:00,  1.92it/s]\n",
            "[epoch 4], [iter 250 / 1172], [train loss 0.6308], [train acc 0.8466]\n",
            " 25%|████████▉                          | 298/1172 [02:34<07:34,  1.92it/s]\n",
            "[epoch 4], [iter 300 / 1172], [train loss 0.6293], [train acc 0.8471]\n",
            " 30%|██████████▍                        | 348/1172 [03:00<06:59,  1.97it/s]\n",
            "[epoch 4], [iter 350 / 1172], [train loss 0.6231], [train acc 0.8487]\n",
            " 34%|███████████▉                       | 398/1172 [03:25<06:36,  1.95it/s]\n",
            "[epoch 4], [iter 400 / 1172], [train loss 0.6169], [train acc 0.8501]\n",
            " 38%|█████████████▍                     | 448/1172 [03:51<06:11,  1.95it/s]\n",
            "[epoch 4], [iter 450 / 1172], [train loss 0.6183], [train acc 0.8503]\n",
            " 42%|██████████████▊                    | 498/1172 [04:17<05:46,  1.94it/s]\n",
            "[epoch 4], [iter 500 / 1172], [train loss 0.6123], [train acc 0.8518]\n",
            " 47%|████████████████▎                  | 548/1172 [04:42<05:19,  1.95it/s]\n",
            "[epoch 4], [iter 550 / 1172], [train loss 0.6065], [train acc 0.8532]\n",
            " 51%|█████████████████▊                 | 598/1172 [05:08<04:55,  1.94it/s]\n",
            "[epoch 4], [iter 600 / 1172], [train loss 0.6093], [train acc 0.8530]\n",
            " 55%|███████████████████▎               | 648/1172 [05:34<04:28,  1.95it/s]\n",
            "[epoch 4], [iter 650 / 1172], [train loss 0.6036], [train acc 0.8544]\n",
            " 60%|████████████████████▊              | 698/1172 [06:00<04:01,  1.96it/s]\n",
            "[epoch 4], [iter 700 / 1172], [train loss 0.5981], [train acc 0.8557]\n",
            " 64%|██████████████████████▎            | 748/1172 [06:25<03:38,  1.94it/s]\n",
            "[epoch 4], [iter 750 / 1172], [train loss 0.6049], [train acc 0.8547]\n",
            " 68%|███████████████████████▊           | 798/1172 [06:51<03:10,  1.96it/s]\n",
            "[epoch 4], [iter 800 / 1172], [train loss 0.5994], [train acc 0.8561]\n",
            " 72%|█████████████████████████▎         | 848/1172 [07:17<02:45,  1.95it/s]\n",
            "[epoch 4], [iter 850 / 1172], [train loss 0.5940], [train acc 0.8574]\n",
            " 77%|██████████████████████████▊        | 898/1172 [07:42<02:20,  1.95it/s]\n",
            "[epoch 4], [iter 900 / 1172], [train loss 0.6030], [train acc 0.8556]\n",
            " 81%|████████████████████████████▎      | 948/1172 [08:08<01:55,  1.95it/s]\n",
            "[epoch 4], [iter 950 / 1172], [train loss 0.5987], [train acc 0.8569]\n",
            " 85%|█████████████████████████████▊     | 998/1172 [08:34<01:28,  1.96it/s]\n",
            "[epoch 4], [iter 1000 / 1172], [train loss 0.5935], [train acc 0.8581]\n",
            " 89%|██████████████████████████████▍   | 1048/1172 [09:00<01:03,  1.96it/s]\n",
            "[epoch 4], [iter 1050 / 1172], [train loss 0.6088], [train acc 0.8542]\n",
            " 94%|███████████████████████████████▊  | 1098/1172 [09:25<00:38,  1.92it/s]\n",
            "[epoch 4], [iter 1100 / 1172], [train loss 0.6038], [train acc 0.8555]\n",
            " 98%|█████████████████████████████████▎| 1148/1172 [09:51<00:12,  1.95it/s]\n",
            "[epoch 4], [iter 1150 / 1172], [train loss 0.5987], [train acc 0.8567]\n",
            "\t>>> Training: Loss 0.5964, Reg 0.000, Acc 0.8573, precision: 0.8573, recall0.8573\n",
            "\t>>> Val.    : Loss 12.0655, Reg 0.000, Acc 0.0144, precision: 0.0144, recall0.0144\n",
            "Epoch 5: Best val loss 10.5636, Best val acc 0.0144, best val recall 0.0144, best val precision 0.0144\n",
            "  4%|█▍                                  | 48/1172 [00:25<09:41,  1.93it/s]\n",
            "[epoch 5], [iter 50 / 1172], [train loss 0.6100], [train acc 0.8536]\n",
            "  8%|███                                 | 98/1172 [00:51<09:09,  1.95it/s]\n",
            "[epoch 5], [iter 100 / 1172], [train loss 0.6219], [train acc 0.8497]\n",
            " 13%|████▍                              | 148/1172 [01:17<08:46,  1.95it/s]\n",
            "[epoch 5], [iter 150 / 1172], [train loss 0.6214], [train acc 0.8510]\n",
            " 17%|█████▉                             | 198/1172 [01:42<08:21,  1.94it/s]\n",
            "[epoch 5], [iter 200 / 1172], [train loss 0.6171], [train acc 0.8522]\n",
            " 21%|███████▍                           | 248/1172 [02:08<07:55,  1.94it/s]\n",
            "[epoch 5], [iter 250 / 1172], [train loss 0.6148], [train acc 0.8530]\n",
            " 25%|████████▉                          | 298/1172 [02:34<07:27,  1.95it/s]\n",
            "[epoch 5], [iter 300 / 1172], [train loss 0.6354], [train acc 0.8462]\n",
            " 30%|██████████▍                        | 348/1172 [02:59<07:04,  1.94it/s]\n",
            "[epoch 5], [iter 350 / 1172], [train loss 0.6393], [train acc 0.8430]\n",
            " 34%|███████████▉                       | 398/1172 [03:25<06:35,  1.96it/s]\n",
            "[epoch 5], [iter 400 / 1172], [train loss 0.6346], [train acc 0.8443]\n",
            " 38%|█████████████▍                     | 448/1172 [03:51<06:11,  1.95it/s]\n",
            "[epoch 5], [iter 450 / 1172], [train loss 0.6345], [train acc 0.8445]\n",
            " 42%|██████████████▊                    | 498/1172 [04:17<05:47,  1.94it/s]\n",
            "[epoch 5], [iter 500 / 1172], [train loss 0.6296], [train acc 0.8457]\n",
            " 47%|████████████████▎                  | 548/1172 [04:42<05:22,  1.93it/s]\n",
            "[epoch 5], [iter 550 / 1172], [train loss 0.6246], [train acc 0.8469]\n",
            " 51%|█████████████████▊                 | 598/1172 [05:08<04:53,  1.95it/s]\n",
            "[epoch 5], [iter 600 / 1172], [train loss 0.6262], [train acc 0.8468]\n",
            " 55%|███████████████████▎               | 648/1172 [05:34<04:27,  1.96it/s]\n",
            "[epoch 5], [iter 650 / 1172], [train loss 0.6214], [train acc 0.8480]\n",
            " 60%|████████████████████▊              | 698/1172 [05:59<04:03,  1.95it/s]\n",
            "[epoch 5], [iter 700 / 1172], [train loss 0.6166], [train acc 0.8492]\n",
            " 64%|██████████████████████▎            | 748/1172 [06:25<03:36,  1.96it/s]\n",
            "[epoch 5], [iter 750 / 1172], [train loss 0.6209], [train acc 0.8484]\n",
            " 68%|███████████████████████▊           | 798/1172 [06:50<03:11,  1.95it/s]\n",
            "[epoch 5], [iter 800 / 1172], [train loss 0.6163], [train acc 0.8496]\n",
            " 72%|█████████████████████████▎         | 848/1172 [07:16<02:45,  1.95it/s]\n",
            "[epoch 5], [iter 850 / 1172], [train loss 0.6117], [train acc 0.8507]\n",
            " 77%|██████████████████████████▊        | 898/1172 [07:42<02:20,  1.95it/s]\n",
            "[epoch 5], [iter 900 / 1172], [train loss 0.6166], [train acc 0.8498]\n",
            " 81%|████████████████████████████▎      | 948/1172 [08:07<01:54,  1.96it/s]\n",
            "[epoch 5], [iter 950 / 1172], [train loss 0.6122], [train acc 0.8509]\n",
            " 85%|█████████████████████████████▊     | 998/1172 [08:33<01:30,  1.91it/s]\n",
            "[epoch 5], [iter 1000 / 1172], [train loss 0.6077], [train acc 0.8520]\n",
            " 89%|██████████████████████████████▍   | 1048/1172 [08:59<01:03,  1.95it/s]\n",
            "[epoch 5], [iter 1050 / 1172], [train loss 0.6147], [train acc 0.8508]\n",
            " 94%|███████████████████████████████▊  | 1098/1172 [09:24<00:37,  1.95it/s]\n",
            "[epoch 5], [iter 1100 / 1172], [train loss 0.6104], [train acc 0.8518]\n",
            " 98%|█████████████████████████████████▎| 1148/1172 [09:50<00:12,  1.95it/s]\n",
            "[epoch 5], [iter 1150 / 1172], [train loss 0.6060], [train acc 0.8529]\n",
            "\t>>> Training: Loss 0.6041, Reg 0.000, Acc 0.8534, precision: 0.8534, recall0.8534\n",
            "\t>>> Val.    : Loss 11.5780, Reg 0.000, Acc 0.0144, precision: 0.0144, recall0.0144\n",
            "Epoch 6: Best val loss 10.5636, Best val acc 0.0144, best val recall 0.0144, best val precision 0.0144\n",
            "  4%|█▍                                  | 48/1172 [00:25<09:52,  1.90it/s]\n",
            "[epoch 6], [iter 50 / 1172], [train loss 0.6224], [train acc 0.8477]\n",
            "  8%|███                                 | 98/1172 [00:51<09:06,  1.96it/s]\n",
            "[epoch 6], [iter 100 / 1172], [train loss 0.6320], [train acc 0.8442]\n",
            " 13%|████▍                              | 148/1172 [01:17<08:45,  1.95it/s]\n",
            "[epoch 6], [iter 150 / 1172], [train loss 0.6321], [train acc 0.8450]\n",
            " 17%|█████▉                             | 198/1172 [01:42<08:21,  1.94it/s]\n",
            "[epoch 6], [iter 200 / 1172], [train loss 0.6279], [train acc 0.8461]\n",
            " 21%|███████▍                           | 248/1172 [02:08<08:11,  1.88it/s]\n",
            "[epoch 6], [iter 250 / 1172], [train loss 0.6264], [train acc 0.8468]\n",
            " 25%|████████▉                          | 298/1172 [02:34<07:28,  1.95it/s]\n",
            "[epoch 6], [iter 300 / 1172], [train loss 0.6389], [train acc 0.8412]\n",
            " 30%|██████████▍                        | 348/1172 [02:59<06:59,  1.96it/s]\n",
            "[epoch 6], [iter 350 / 1172], [train loss 0.6389], [train acc 0.8419]\n",
            " 34%|███████████▉                       | 398/1172 [03:25<06:35,  1.96it/s]\n",
            "[epoch 6], [iter 400 / 1172], [train loss 0.6352], [train acc 0.8430]\n",
            " 38%|█████████████▍                     | 448/1172 [03:51<06:12,  1.94it/s]\n",
            "[epoch 6], [iter 450 / 1172], [train loss 0.6357], [train acc 0.8428]\n",
            " 42%|██████████████▊                    | 498/1172 [04:17<05:50,  1.92it/s]\n",
            "[epoch 6], [iter 500 / 1172], [train loss 0.6315], [train acc 0.8438]\n",
            " 47%|████████████████▎                  | 548/1172 [04:43<05:21,  1.94it/s]\n",
            "[epoch 6], [iter 550 / 1172], [train loss 0.6273], [train acc 0.8448]\n",
            " 51%|█████████████████▊                 | 598/1172 [05:09<04:55,  1.95it/s]\n",
            "[epoch 6], [iter 600 / 1172], [train loss 0.6315], [train acc 0.8440]\n",
            " 55%|███████████████████▎               | 648/1172 [05:35<04:28,  1.95it/s]\n",
            "[epoch 6], [iter 650 / 1172], [train loss 0.6273], [train acc 0.8451]\n",
            " 60%|████████████████████▊              | 698/1172 [06:00<04:06,  1.92it/s]\n",
            "[epoch 6], [iter 700 / 1172], [train loss 0.6233], [train acc 0.8461]\n",
            " 64%|██████████████████████▎            | 748/1172 [06:26<03:38,  1.94it/s]\n",
            "[epoch 6], [iter 750 / 1172], [train loss 0.6274], [train acc 0.8457]\n",
            " 68%|███████████████████████▊           | 798/1172 [06:52<03:14,  1.92it/s]\n",
            "[epoch 6], [iter 800 / 1172], [train loss 0.6234], [train acc 0.8467]\n",
            " 72%|█████████████████████████▎         | 848/1172 [07:18<02:46,  1.94it/s]\n",
            "[epoch 6], [iter 850 / 1172], [train loss 0.6194], [train acc 0.8476]\n",
            " 77%|██████████████████████████▊        | 898/1172 [07:44<02:20,  1.94it/s]\n",
            "[epoch 6], [iter 900 / 1172], [train loss 0.6248], [train acc 0.8469]\n",
            " 81%|████████████████████████████▎      | 948/1172 [08:10<01:55,  1.95it/s]\n",
            "[epoch 6], [iter 950 / 1172], [train loss 0.6212], [train acc 0.8479]\n",
            " 85%|█████████████████████████████▊     | 998/1172 [08:35<01:30,  1.93it/s]\n",
            "[epoch 6], [iter 1000 / 1172], [train loss 0.6174], [train acc 0.8488]\n",
            " 89%|██████████████████████████████▍   | 1048/1172 [09:01<01:06,  1.88it/s]\n",
            "[epoch 6], [iter 1050 / 1172], [train loss 0.6294], [train acc 0.8460]\n",
            " 94%|███████████████████████████████▊  | 1098/1172 [09:27<00:37,  1.95it/s]\n",
            "[epoch 6], [iter 1100 / 1172], [train loss 0.6270], [train acc 0.8469]\n",
            " 98%|█████████████████████████████████▎| 1148/1172 [09:53<00:12,  1.94it/s]\n",
            "[epoch 6], [iter 1150 / 1172], [train loss 0.6232], [train acc 0.8479]\n",
            "\t>>> Training: Loss 0.6214, Reg 0.000, Acc 0.8483, precision: 0.8483, recall0.8483\n",
            "\t>>> Val.    : Loss 11.5727, Reg 0.000, Acc 0.0144, precision: 0.0144, recall0.0144\n",
            "Epoch 7: Best val loss 10.5636, Best val acc 0.0144, best val recall 0.0144, best val precision 0.0144\n",
            "  4%|█▍                                  | 48/1172 [00:25<09:48,  1.91it/s]\n",
            "[epoch 7], [iter 50 / 1172], [train loss 0.6410], [train acc 0.8433]\n",
            "  8%|███                                 | 98/1172 [00:51<09:15,  1.93it/s]\n",
            "[epoch 7], [iter 100 / 1172], [train loss 0.6520], [train acc 0.8398]\n",
            " 13%|████▍                              | 148/1172 [01:17<08:50,  1.93it/s]\n",
            "[epoch 7], [iter 150 / 1172], [train loss 0.6530], [train acc 0.8396]\n",
            " 17%|█████▉                             | 198/1172 [01:42<08:21,  1.94it/s]\n",
            "[epoch 7], [iter 200 / 1172], [train loss 0.6494], [train acc 0.8406]\n",
            " 21%|███████▍                           | 248/1172 [02:08<07:59,  1.93it/s]\n",
            "[epoch 7], [iter 250 / 1172], [train loss 0.6477], [train acc 0.8412]\n",
            " 25%|████████▉                          | 298/1172 [02:34<07:30,  1.94it/s]\n",
            "[epoch 7], [iter 300 / 1172], [train loss 0.6575], [train acc 0.8376]\n",
            " 30%|██████████▍                        | 348/1172 [02:59<06:58,  1.97it/s]\n",
            "[epoch 7], [iter 350 / 1172], [train loss 0.6551], [train acc 0.8385]\n",
            " 34%|███████████▉                       | 398/1172 [03:24<06:36,  1.95it/s]\n",
            "[epoch 7], [iter 400 / 1172], [train loss 0.6516], [train acc 0.8395]\n",
            " 38%|█████████████▍                     | 448/1172 [03:50<06:10,  1.96it/s]\n",
            "[epoch 7], [iter 450 / 1172], [train loss 0.6620], [train acc 0.8364]\n",
            " 42%|██████████████▊                    | 498/1172 [04:16<05:43,  1.96it/s]\n",
            "[epoch 7], [iter 500 / 1172], [train loss 0.6588], [train acc 0.8373]\n",
            " 47%|████████████████▎                  | 548/1172 [04:41<05:19,  1.95it/s]\n",
            "[epoch 7], [iter 550 / 1172], [train loss 0.6551], [train acc 0.8382]\n",
            " 51%|█████████████████▊                 | 598/1172 [05:07<04:59,  1.92it/s]\n",
            "[epoch 7], [iter 600 / 1172], [train loss 0.6639], [train acc 0.8356]\n",
            " 55%|███████████████████▎               | 648/1172 [05:33<04:24,  1.98it/s]\n",
            "[epoch 7], [iter 650 / 1172], [train loss 0.6608], [train acc 0.8365]\n",
            " 60%|████████████████████▊              | 698/1172 [05:58<04:01,  1.96it/s]\n",
            "[epoch 7], [iter 700 / 1172], [train loss 0.6571], [train acc 0.8374]\n",
            " 64%|██████████████████████▎            | 748/1172 [06:24<03:37,  1.95it/s]\n",
            "[epoch 7], [iter 750 / 1172], [train loss 0.6626], [train acc 0.8358]\n",
            " 68%|███████████████████████▊           | 798/1172 [06:49<03:11,  1.96it/s]\n",
            "[epoch 7], [iter 800 / 1172], [train loss 0.6590], [train acc 0.8367]\n",
            " 72%|█████████████████████████▎         | 848/1172 [07:15<02:44,  1.97it/s]\n",
            "[epoch 7], [iter 850 / 1172], [train loss 0.6554], [train acc 0.8376]\n",
            " 77%|██████████████████████████▊        | 898/1172 [07:40<02:21,  1.94it/s]\n",
            "[epoch 7], [iter 900 / 1172], [train loss 0.6599], [train acc 0.8365]\n",
            " 81%|████████████████████████████▎      | 948/1172 [08:06<01:53,  1.97it/s]\n",
            "[epoch 7], [iter 950 / 1172], [train loss 0.6567], [train acc 0.8374]\n",
            " 85%|█████████████████████████████▊     | 998/1172 [08:31<01:28,  1.97it/s]\n",
            "[epoch 7], [iter 1000 / 1172], [train loss 0.6531], [train acc 0.8383]\n",
            " 89%|██████████████████████████████▍   | 1048/1172 [08:57<01:02,  1.97it/s]\n",
            "[epoch 7], [iter 1050 / 1172], [train loss 0.6605], [train acc 0.8363]\n",
            " 94%|███████████████████████████████▊  | 1098/1172 [09:22<00:37,  1.96it/s]\n",
            "[epoch 7], [iter 1100 / 1172], [train loss 0.6576], [train acc 0.8372]\n",
            " 98%|█████████████████████████████████▎| 1148/1172 [09:48<00:12,  1.95it/s]\n",
            "[epoch 7], [iter 1150 / 1172], [train loss 0.6542], [train acc 0.8380]\n",
            "\t>>> Training: Loss 0.6526, Reg 0.000, Acc 0.8384, precision: 0.8384, recall0.8384\n",
            "\t>>> Val.    : Loss 11.1612, Reg 0.000, Acc 0.0144, precision: 0.0144, recall0.0144\n",
            "Epoch 8: Best val loss 10.5636, Best val acc 0.0144, best val recall 0.0144, best val precision 0.0144\n",
            "  4%|█▍                                  | 48/1172 [00:25<09:25,  1.99it/s]\n",
            "[epoch 8], [iter 50 / 1172], [train loss 0.6650], [train acc 0.8345]\n",
            "  8%|███                                 | 98/1172 [00:50<09:03,  1.98it/s]\n",
            "[epoch 8], [iter 100 / 1172], [train loss 0.6709], [train acc 0.8317]\n",
            " 13%|████▍                              | 148/1172 [01:16<08:36,  1.98it/s]\n",
            "[epoch 8], [iter 150 / 1172], [train loss 0.6675], [train acc 0.8326]\n",
            " 17%|█████▉                             | 198/1172 [01:41<08:11,  1.98it/s]\n",
            "[epoch 8], [iter 200 / 1172], [train loss 0.6640], [train acc 0.8335]\n",
            " 21%|███████▍                           | 248/1172 [02:07<07:55,  1.94it/s]\n",
            "[epoch 8], [iter 250 / 1172], [train loss 0.6632], [train acc 0.8341]\n",
            " 25%|████████▉                          | 298/1172 [02:32<07:20,  1.98it/s]\n",
            "[epoch 8], [iter 300 / 1172], [train loss 0.6650], [train acc 0.8337]\n",
            " 30%|██████████▍                        | 348/1172 [02:57<07:03,  1.95it/s]\n",
            "[epoch 8], [iter 350 / 1172], [train loss 0.6616], [train acc 0.8346]\n",
            " 34%|███████████▉                       | 398/1172 [03:23<06:38,  1.94it/s]\n",
            "[epoch 8], [iter 400 / 1172], [train loss 0.6582], [train acc 0.8354]\n",
            " 38%|█████████████▍                     | 448/1172 [03:48<06:11,  1.95it/s]\n",
            "[epoch 8], [iter 450 / 1172], [train loss 0.6599], [train acc 0.8351]\n",
            " 42%|██████████████▊                    | 498/1172 [04:14<05:42,  1.97it/s]\n",
            "[epoch 8], [iter 500 / 1172], [train loss 0.6566], [train acc 0.8359]\n",
            " 47%|████████████████▎                  | 548/1172 [04:40<05:17,  1.97it/s]\n",
            "[epoch 8], [iter 550 / 1172], [train loss 0.6533], [train acc 0.8368]\n",
            " 51%|█████████████████▊                 | 598/1172 [05:05<04:59,  1.91it/s]\n",
            "[epoch 8], [iter 600 / 1172], [train loss 0.6546], [train acc 0.8367]\n",
            " 55%|███████████████████▎               | 648/1172 [05:31<04:25,  1.97it/s]\n",
            "[epoch 8], [iter 650 / 1172], [train loss 0.6513], [train acc 0.8375]\n",
            " 60%|████████████████████▊              | 698/1172 [05:56<04:00,  1.97it/s]\n",
            "[epoch 8], [iter 700 / 1172], [train loss 0.6481], [train acc 0.8383]\n",
            " 62%|█████████████████████▋             | 728/1172 [06:11<03:46,  1.96it/s]"
          ]
        }
      ],
      "source": [
        "!python main.py --use_reg_loss False --base resnext101_32x8d --dataset isic2018 --hidden_dim 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4m8fvzN-oJN"
      },
      "outputs": [],
      "source": [
        "#%mkdir /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/model_checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UULfzcFrynFz",
        "outputId": "3afcec1a-3b79-4a39-b9da-eb0742c4ba00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hFw2rQB-7np"
      },
      "outputs": [],
      "source": [
        "#%cp ./saved/model/epoch97_acc_0.762.ckpt /content/drive/MyDrive/Corbin_Adam_PhD_Workspace/corbin_papers/dissertation_proposal/model_checkpoints/CIRCLE/mobilenetv3l/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1_HSnGPDTmtQqL19RJaSZ9opYOjST_pFh",
      "authorship_tag": "ABX9TyNzLvOmZ0W+YlZztiCKgjkl",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}